{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lF_YzgAFBUy"
      },
      "source": [
        "# Homework 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlUZyP2xFCZx"
      },
      "source": [
        "In this homework, you will test various time series classification methods. You must choose **three** datasets from [the UCR/UEA time series repository](http://timeseriesclassification.com) and perform the tasks by evaluating the models on three selected datasets.\n",
        "\n",
        "Note that the questions are not 100% the same as the lab tasks. Please carefully read all descriptions. Compared to previous homework, there is no fixed answer, and we will evaluate your assignment based on your trials rather than the results. When the description does not specify or restrict things during your implementation process, you can choose your way freely. We will always grade based on the written description on each task only.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va23r8yGIfjW"
      },
      "source": [
        "### Task 0: Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2oklHd9Igso"
      },
      "source": [
        "You need to choose **three** datasets from the UCR/UEA time series repository. Please be careful since some UCR datasets can take a long time to be processed - You do not need to choose heavy datasets since it would slow your training/testing process. Use sktime's `load_UCR_UEA_dataset` function to perform. Please note that **you should use each dataset's original train/test splits** to train and report the test scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsP1q2UuIdwo"
      },
      "outputs": [],
      "source": [
        "from sktime.datasets import load_UCR_UEA_dataset\n",
        "X_train_ecg200, y_train_ecg200 = load_UCR_UEA_dataset(name=\"ECG200\", split=\"train\", return_X_y=True, return_type=\"numpy3D\")\n",
        "X_test_ecg200, y_test_ecg200 = load_UCR_UEA_dataset(name=\"ECG200\", split=\"test\", return_X_y=True, return_type=\"numpy3D\")\n",
        "\n",
        "#coffee dataset\n",
        "X_train_coffee, y_train_coffee = load_UCR_UEA_dataset(name=\"Coffee\", split=\"train\", return_X_y=True, return_type=\"numpy3D\")\n",
        "X_test_coffee, y_test_coffee = load_UCR_UEA_dataset(name=\"Coffee\", split=\"test\", return_X_y=True, return_type=\"numpy3D\")\n",
        "\n",
        "\n",
        "X_train_wafer, y_train_wafer = load_UCR_UEA_dataset(\"Wafer\", split=\"train\", return_X_y=True,return_type=\"numpy3D\")\n",
        "X_test_wafer, y_test_wafer = load_UCR_UEA_dataset(\"Wafer\", split=\"test\", return_X_y=True,return_type=\"numpy3D\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxZ3wcr0ZZKr",
        "outputId": "81e6338a-a6f0-4b9b-fb36-7ffdb16b4e3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sktime in /usr/local/lib/python3.10/dist-packages (0.27.0)\n",
            "Requirement already satisfied: numpy<1.27,>=1.21 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sktime) (24.0)\n",
            "Requirement already satisfied: pandas<2.2.0,>=1.1 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.5.3)\n",
            "Requirement already satisfied: scikit-base<0.8.0 in /usr/local/lib/python3.10/dist-packages (from sktime) (0.7.5)\n",
            "Requirement already satisfied: scikit-learn<1.5.0,>=0.24 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.2.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.11.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.0,>=1.1->sktime) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.0,>=1.1->sktime) (2023.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.5.0,>=0.24->sktime) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.5.0,>=0.24->sktime) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<2.2.0,>=1.1->sktime) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sktime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2NHZckcFUuu"
      },
      "source": [
        "### Task 1: Time series classification using deep learning 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT53IJowFR6e"
      },
      "source": [
        "\n",
        "Time series classification problems can be solved using networks like CNN, RNN, or FCN. You can even try to merge different networks. In this task, you must test your three chosen datasets on four other models.\n",
        "\n",
        "Try to implement four different models: 1) Fully connected network at least with five dense layers, 2) One directional RNN, 3) 1D-CNN only, and 4) 1D-CNN +\n",
        "GRU. Note that you can always link the network to a fully connected layer to match the output size. You can freely construct any structure you want. Report the average test scores of four models on three datasets you chose. It would be four scores in total. Mark the best model in terms of the average test score. Briefly explain the structure you constructed. There is no definitive answer, and it is up to your own model. Here it would be best if you keep the following rules:\n",
        "\n",
        "- When initially loading the dataset, use sktime's `load_UCR_UEA_dataset` function. This is for our grading purpose.\n",
        "- You should use at least **two** Tensorflow callbacks when you fit your model. These can be built-in ones or your personalized callback.\n",
        "- You should use Tensorflow's data API (`tf.data`) to manage your dataset and use `shuffle`, `batch,` and `prefetch` functions. This means that you need to convert the data format using the `from_tensor_slices` function. This also means that you need to create your own validation set. You are not limited to using any methods to do this, but you may also need to shuffle the dataset before (for that, you can use `np.random.permutation`). If you use Torch, explain how you implement the equivalent operations.\n",
        "- You need to clearly report the **test accuracy** of the four models. Training and validation accuracy scores are not enough.\n",
        "- You may need to deal with datasets of different sizes. In this case, it might be helpful to make a function to create a model that can receive different input sizes as a parameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8H81XkUPelrM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "X_train_ecg200, y_train_ecg200 = load_UCR_UEA_dataset(name=\"ECG200\", split=\"train\", return_X_y=True, return_type=\"numpy3D\")\n",
        "X_test_ecg200, y_test_ecg200 = load_UCR_UEA_dataset(name=\"ECG200\", split=\"test\", return_X_y=True, return_type=\"numpy3D\")\n",
        "\n",
        "if not isinstance(y_train_ecg200, np.ndarray):\n",
        "    y_train_ecg200 = np.array(y_train_ecg200)\n",
        "if not isinstance(y_test_ecg200, np.ndarray):\n",
        "    y_test_ecg200 = np.array(y_test_ecg200)\n",
        "\n",
        "\n",
        "y_train_ecg200 = y_train_ecg200.astype('int64')\n",
        "y_test_ecg200 = y_test_ecg200.astype('int64')\n",
        "\n",
        "def prepare_dataset(X, y, batch_size=32, shuffle_buffer_size=100):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    if shuffle_buffer_size > 0:\n",
        "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "batch_size = 32\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNATucqJelrM",
        "outputId": "38375ea3-bfd0-4ceb-f908-35747a5c9651"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of NaN values in X_train:  0\n",
            "Number of NaN values in X_test:  0\n",
            "Number of NaN values in y_train:  0\n",
            "Number of NaN values in y_test:  0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Number of NaN values in X_train: \", np.isnan(X_train_ecg200).sum())\n",
        "print(\"Number of NaN values in X_test: \", np.isnan(X_test_ecg200).sum())\n",
        "print(\"Number of NaN values in y_train: \", np.isnan(y_train_ecg200).sum())\n",
        "print(\"Number of NaN values in y_test: \", np.isnan(y_test_ecg200).sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soxa8TN5elrM"
      },
      "outputs": [],
      "source": [
        "\n",
        "mean = X_train_ecg200.mean()\n",
        "std = X_train_ecg200.std()\n",
        "\n",
        "X_train_normalized = (X_train_ecg200 - mean) / std\n",
        "X_test_normalized = (X_test_ecg200 - mean) / std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7Cx9IC1elrN",
        "outputId": "34dce491-2229-458c-d562-db6051e0b795"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((100, 1, 96), (100, 1, 96))"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_normalized.shape, X_test_normalized.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTQz2lsuelrN",
        "outputId": "07cd02f1-e588-416d-c7de-a11ffc6c2d0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-1,  1])"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.unique(y_train_ecg200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BI_yxTF8elrN"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_train_ecg200[y_train_ecg200 == -1] = 0\n",
        "y_test_ecg200[y_test_ecg200 == -1] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-6G0o95elrO"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, Conv1D, GRU, Flatten, Input\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "# 1. Fully Connected Network\n",
        "def create_fcn_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Input(shape=input_shape),\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# 2. One Directional RNN\n",
        "def create_rnn_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        SimpleRNN(50, input_shape=input_shape, return_sequences=True),\n",
        "        SimpleRNN(50),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# 3. 1D-CNN\n",
        "def create_cnn_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Input(shape=input_shape),\n",
        "        Conv1D(filters=16, kernel_size=3, activation='relu'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Flatten(),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# 4. 1D-CNN + GRU\n",
        "def create_cnn_gru_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
        "        Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "        GRU(50),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9HwzUfbelrO"
      },
      "outputs": [],
      "source": [
        "# Reshape the data to (samples, timesteps, features)\n",
        "X_train_ecg200 = X_train_normalized.reshape((-1, 96, 1))\n",
        "X_test_ecg200 = X_test_normalized.reshape((-1, 96, 1))\n",
        "\n",
        "train_dataset_ecg200 = prepare_dataset(X_train_ecg200, y_train_ecg200, batch_size=batch_size)\n",
        "test_dataset_ecg200 = prepare_dataset(X_test_ecg200, y_test_ecg200, batch_size=batch_size)\n",
        "\n",
        "input_shape_3d = (96, 1)  # 96 timesteps, 1 feature\n",
        "\n",
        "num_classes = len(np.unique(y_train_ecg200))\n",
        "fcn_model = create_fcn_model(input_shape_3d, num_classes)\n",
        "rnn_model = create_rnn_model(input_shape_3d, num_classes)\n",
        "cnn_model = create_cnn_model(input_shape_3d, num_classes)\n",
        "cnn_gru_model = create_cnn_gru_model(input_shape_3d, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMY4bIlXelrO"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "def train_model(model, train_dataset, test_dataset, epochs=10, learning_rate=1e-5):\n",
        "    # Check for NaNs just before training\n",
        "    def check_nan(dataset):\n",
        "        for features, labels in dataset:\n",
        "            if tf.reduce_any(tf.math.is_nan(features)):\n",
        "                raise ValueError(\"NaN values detected in dataset.\")\n",
        "\n",
        "    check_nan(train_dataset)\n",
        "    check_nan(test_dataset)\n",
        "\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1),\n",
        "        tf.keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "    ]\n",
        "\n",
        "    optimizer = Adam(learning_rate=learning_rate, clipvalue=0.5)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        epochs=epochs,\n",
        "        validation_data=test_dataset,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Load the best weights, if available\n",
        "    try:\n",
        "        model.load_weights('best_model.h5')\n",
        "    except FileNotFoundError:\n",
        "        print(\"Checkpoint file not found. Using model as is after training.\")\n",
        "\n",
        "    test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "    return history, test_loss, test_accuracy\n",
        "\n",
        "learning_rate=1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHdGXW4XelrP",
        "outputId": "41af874e-2100-4016-b6b4-9e4bbba9456a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1/4 [======>.......................] - ETA: 4s - loss: 0.6840 - accuracy: 0.5312\n",
            "Epoch 1: val_loss improved from inf to 0.69020, saving model to best_model.h5\n",
            "4/4 [==============================] - 2s 86ms/step - loss: 0.6926 - accuracy: 0.5200 - val_loss: 0.6902 - val_accuracy: 0.5800\n",
            "Epoch 2/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6838 - accuracy: 0.5625\n",
            "Epoch 2: val_loss improved from 0.69020 to 0.68377, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.6837 - accuracy: 0.6600 - val_loss: 0.6838 - val_accuracy: 0.6400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6748 - accuracy: 0.7500\n",
            "Epoch 3: val_loss improved from 0.68377 to 0.67770, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.6766 - accuracy: 0.7200 - val_loss: 0.6777 - val_accuracy: 0.6500\n",
            "Epoch 4/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6695 - accuracy: 0.8125\n",
            "Epoch 4: val_loss improved from 0.67770 to 0.67233, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.6697 - accuracy: 0.7200 - val_loss: 0.6723 - val_accuracy: 0.6500\n",
            "Epoch 5/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6830 - accuracy: 0.6875\n",
            "Epoch 5: val_loss improved from 0.67233 to 0.66769, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.6632 - accuracy: 0.7200 - val_loss: 0.6677 - val_accuracy: 0.6500\n",
            "Epoch 6/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6625 - accuracy: 0.6875\n",
            "Epoch 6: val_loss improved from 0.66769 to 0.66318, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.6578 - accuracy: 0.7200 - val_loss: 0.6632 - val_accuracy: 0.6500\n",
            "Epoch 7/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6595 - accuracy: 0.6562\n",
            "Epoch 7: val_loss improved from 0.66318 to 0.65870, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 42ms/step - loss: 0.6522 - accuracy: 0.7200 - val_loss: 0.6587 - val_accuracy: 0.6600\n",
            "Epoch 8/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6405 - accuracy: 0.8438\n",
            "Epoch 8: val_loss improved from 0.65870 to 0.65441, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 43ms/step - loss: 0.6467 - accuracy: 0.7200 - val_loss: 0.6544 - val_accuracy: 0.6600\n",
            "Epoch 9/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6629 - accuracy: 0.6875\n",
            "Epoch 9: val_loss improved from 0.65441 to 0.65027, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 0.6415 - accuracy: 0.7200 - val_loss: 0.6503 - val_accuracy: 0.6600\n",
            "Epoch 10/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6296 - accuracy: 0.7500\n",
            "Epoch 10: val_loss improved from 0.65027 to 0.64629, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 0.6364 - accuracy: 0.7200 - val_loss: 0.6463 - val_accuracy: 0.6800\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6463 - accuracy: 0.6800\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate each model\n",
        "fcn_history, fcn_test_loss, fcn_accuracy = train_model(fcn_model, train_dataset_ecg200, test_dataset_ecg200, epochs=10, learning_rate=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWiAYX3XelrP",
        "outputId": "5f06b139-0c8e-4328-9556-fa090fbab407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6878 - accuracy: 0.5729\n",
            "Epoch 1: val_loss improved from inf to 0.67654, saving model to best_model.h5\n",
            "4/4 [==============================] - 3s 185ms/step - loss: 0.6852 - accuracy: 0.5800 - val_loss: 0.6765 - val_accuracy: 0.6300\n",
            "Epoch 2/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6791 - accuracy: 0.6250\n",
            "Epoch 2: val_loss improved from 0.67654 to 0.67170, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 75ms/step - loss: 0.6780 - accuracy: 0.6200 - val_loss: 0.6717 - val_accuracy: 0.6500\n",
            "Epoch 3/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6745 - accuracy: 0.6354\n",
            "Epoch 3: val_loss improved from 0.67170 to 0.66705, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 72ms/step - loss: 0.6712 - accuracy: 0.6400 - val_loss: 0.6671 - val_accuracy: 0.6800\n",
            "Epoch 4/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6660 - accuracy: 0.6354\n",
            "Epoch 4: val_loss improved from 0.66705 to 0.66243, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 73ms/step - loss: 0.6652 - accuracy: 0.6400 - val_loss: 0.6624 - val_accuracy: 0.6800\n",
            "Epoch 5/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6681 - accuracy: 0.6250\n",
            "Epoch 5: val_loss improved from 0.66243 to 0.65798, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 77ms/step - loss: 0.6588 - accuracy: 0.6400 - val_loss: 0.6580 - val_accuracy: 0.6800\n",
            "Epoch 6/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6554 - accuracy: 0.6250\n",
            "Epoch 6: val_loss improved from 0.65798 to 0.65370, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 72ms/step - loss: 0.6526 - accuracy: 0.6400 - val_loss: 0.6537 - val_accuracy: 0.7100\n",
            "Epoch 7/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6462 - accuracy: 0.6562\n",
            "Epoch 7: val_loss improved from 0.65370 to 0.64944, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 73ms/step - loss: 0.6468 - accuracy: 0.6500 - val_loss: 0.6494 - val_accuracy: 0.7300\n",
            "Epoch 8/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6420 - accuracy: 0.6667\n",
            "Epoch 8: val_loss improved from 0.64944 to 0.64521, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 70ms/step - loss: 0.6408 - accuracy: 0.6700 - val_loss: 0.6452 - val_accuracy: 0.7400\n",
            "Epoch 9/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6347 - accuracy: 0.6771\n",
            "Epoch 9: val_loss improved from 0.64521 to 0.64101, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 67ms/step - loss: 0.6352 - accuracy: 0.6800 - val_loss: 0.6410 - val_accuracy: 0.7500\n",
            "Epoch 10/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6197 - accuracy: 0.7188\n",
            "Epoch 10: val_loss improved from 0.64101 to 0.63694, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 66ms/step - loss: 0.6294 - accuracy: 0.7100 - val_loss: 0.6369 - val_accuracy: 0.7700\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.6369 - accuracy: 0.7700\n"
          ]
        }
      ],
      "source": [
        "rnn_history, rnn_test_loss, rnn_accuracy = train_model(rnn_model, train_dataset_ecg200, test_dataset_ecg200, epochs=10, learning_rate=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-XDITOHelrP",
        "outputId": "0e3cda2e-ea1c-4074-a656-0dea88ee6756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1/4 [======>.......................] - ETA: 2s - loss: 0.7003 - accuracy: 0.5000\n",
            "Epoch 1: val_loss improved from inf to 0.65791, saving model to best_model.h5\n",
            "4/4 [==============================] - 1s 67ms/step - loss: 0.6746 - accuracy: 0.5600 - val_loss: 0.6579 - val_accuracy: 0.6100\n",
            "Epoch 2/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6749 - accuracy: 0.6250\n",
            "Epoch 2: val_loss improved from 0.65791 to 0.65411, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6697 - accuracy: 0.5600 - val_loss: 0.6541 - val_accuracy: 0.6100\n",
            "Epoch 3/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6804 - accuracy: 0.5625\n",
            "Epoch 3: val_loss improved from 0.65411 to 0.65051, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.6648 - accuracy: 0.5700 - val_loss: 0.6505 - val_accuracy: 0.6300\n",
            "Epoch 4/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6411 - accuracy: 0.5625\n",
            "Epoch 4: val_loss improved from 0.65051 to 0.64703, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.6600 - accuracy: 0.5800 - val_loss: 0.6470 - val_accuracy: 0.6300\n",
            "Epoch 5/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6260 - accuracy: 0.6250\n",
            "Epoch 5: val_loss improved from 0.64703 to 0.64346, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.6555 - accuracy: 0.5800 - val_loss: 0.6435 - val_accuracy: 0.6300\n",
            "Epoch 6/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6517 - accuracy: 0.5938\n",
            "Epoch 6: val_loss improved from 0.64346 to 0.63999, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.6512 - accuracy: 0.5900 - val_loss: 0.6400 - val_accuracy: 0.6300\n",
            "Epoch 7/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6461 - accuracy: 0.5312\n",
            "Epoch 7: val_loss improved from 0.63999 to 0.63681, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.6467 - accuracy: 0.6000 - val_loss: 0.6368 - val_accuracy: 0.6400\n",
            "Epoch 8/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6390 - accuracy: 0.6250\n",
            "Epoch 8: val_loss improved from 0.63681 to 0.63369, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6423 - accuracy: 0.6100 - val_loss: 0.6337 - val_accuracy: 0.6400\n",
            "Epoch 9/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.7230 - accuracy: 0.4375\n",
            "Epoch 9: val_loss improved from 0.63369 to 0.63063, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.6384 - accuracy: 0.6000 - val_loss: 0.6306 - val_accuracy: 0.6600\n",
            "Epoch 10/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5508 - accuracy: 0.8125\n",
            "Epoch 10: val_loss improved from 0.63063 to 0.62785, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6342 - accuracy: 0.6000 - val_loss: 0.6279 - val_accuracy: 0.6600\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6279 - accuracy: 0.6600\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    cnn_history, cnn_test_loss, cnn_accuracy = train_model(cnn_model, train_dataset_ecg200, test_dataset_ecg200, epochs=10, learning_rate=learning_rate)\n",
        "except Exception as e:\n",
        "    print(\"An error occurred during training:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGfNZE1AelrP",
        "outputId": "08a6cbe0-e46e-4905-fc8c-33733f55ef89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6964 - accuracy: 0.4062\n",
            "Epoch 1: val_loss improved from inf to 0.69588, saving model to best_model.h5\n",
            "4/4 [==============================] - 4s 329ms/step - loss: 0.6964 - accuracy: 0.4000 - val_loss: 0.6959 - val_accuracy: 0.4300\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.6960 - accuracy: 0.4300\n",
            "Epoch 2: val_loss improved from 0.69588 to 0.69561, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 122ms/step - loss: 0.6960 - accuracy: 0.4300 - val_loss: 0.6956 - val_accuracy: 0.4500\n",
            "Epoch 3/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6956 - accuracy: 0.4375\n",
            "Epoch 3: val_loss improved from 0.69561 to 0.69535, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 86ms/step - loss: 0.6956 - accuracy: 0.4400 - val_loss: 0.6954 - val_accuracy: 0.4500\n",
            "Epoch 4/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6951 - accuracy: 0.4688\n",
            "Epoch 4: val_loss improved from 0.69535 to 0.69508, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 85ms/step - loss: 0.6952 - accuracy: 0.4800 - val_loss: 0.6951 - val_accuracy: 0.4700\n",
            "Epoch 5/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6951 - accuracy: 0.5104\n",
            "Epoch 5: val_loss improved from 0.69508 to 0.69482, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 88ms/step - loss: 0.6948 - accuracy: 0.5300 - val_loss: 0.6948 - val_accuracy: 0.5100\n",
            "Epoch 6/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6946 - accuracy: 0.5417\n",
            "Epoch 6: val_loss improved from 0.69482 to 0.69458, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 87ms/step - loss: 0.6945 - accuracy: 0.5400 - val_loss: 0.6946 - val_accuracy: 0.5200\n",
            "Epoch 7/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6943 - accuracy: 0.5417\n",
            "Epoch 7: val_loss improved from 0.69458 to 0.69435, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 84ms/step - loss: 0.6941 - accuracy: 0.5500 - val_loss: 0.6943 - val_accuracy: 0.5400\n",
            "Epoch 8/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6938 - accuracy: 0.5729\n",
            "Epoch 8: val_loss improved from 0.69435 to 0.69410, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 85ms/step - loss: 0.6938 - accuracy: 0.5800 - val_loss: 0.6941 - val_accuracy: 0.5600\n",
            "Epoch 9/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6932 - accuracy: 0.5833\n",
            "Epoch 9: val_loss improved from 0.69410 to 0.69386, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 87ms/step - loss: 0.6934 - accuracy: 0.5800 - val_loss: 0.6939 - val_accuracy: 0.5800\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.6100\n",
            "Epoch 10: val_loss improved from 0.69386 to 0.69360, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 91ms/step - loss: 0.6930 - accuracy: 0.6100 - val_loss: 0.6936 - val_accuracy: 0.5900\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.6936 - accuracy: 0.5900\n"
          ]
        }
      ],
      "source": [
        "cnn_gru_history, cnn_gru_test_loss, cnn_gru_accuracy = train_model(cnn_gru_model, train_dataset_ecg200, test_dataset_ecg200, epochs=10, learning_rate=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL4uZ9oUgXsV",
        "outputId": "dcfc17ee-b3b9-4a7c-9e7e-ef256cde3b48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fully Connected Network Test Accuracy: 0.6800\n",
            "One Directional RNN Test Accuracy: 0.7700\n",
            "1D-CNN Test Accuracy: 0.6600\n",
            "1D-CNN + GRU Test Accuracy: 0.5900\n"
          ]
        }
      ],
      "source": [
        "fcn_test_loss, fcn_test_accuracy = fcn_model.evaluate(X_test_ecg200, y_test_ecg200, verbose=0)\n",
        "print(f\"Fully Connected Network Test Accuracy: {fcn_test_accuracy:.4f}\")\n",
        "\n",
        "rnn_test_loss, rnn_test_accuracy = rnn_model.evaluate(X_test_ecg200, y_test_ecg200, verbose=0)\n",
        "print(f\"One Directional RNN Test Accuracy: {rnn_test_accuracy:.4f}\")\n",
        "\n",
        "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(X_test_ecg200, y_test_ecg200, verbose=0)\n",
        "print(f\"1D-CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
        "\n",
        "cnn_gru_test_loss, cnn_gru_test_accuracy = cnn_gru_model.evaluate(X_test_ecg200, y_test_ecg200, verbose=0)\n",
        "print(f\"1D-CNN + GRU Test Accuracy: {cnn_gru_test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5uHoBVogliT"
      },
      "outputs": [],
      "source": [
        "#coffee dataset\n",
        "X_train_coffee, y_train_coffee = load_UCR_UEA_dataset(name=\"Coffee\", split=\"train\", return_X_y=True, return_type=\"numpy3D\")\n",
        "X_test_coffee, y_test_coffee = load_UCR_UEA_dataset(name=\"Coffee\", split=\"test\", return_X_y=True, return_type=\"numpy3D\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2E9QiXTxgxSk"
      },
      "outputs": [],
      "source": [
        "if not isinstance(y_train_coffee, np.ndarray):\n",
        "    y_train_coffee = np.array(y_train_coffee)\n",
        "if not isinstance(y_test_coffee, np.ndarray):\n",
        "    y_test_coffee = np.array(y_test_coffee)\n",
        "\n",
        "\n",
        "y_train_coffee = y_train_coffee.astype('int64')\n",
        "y_test_coffee = y_test_coffee.astype('int64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbDBDFt1h_UU",
        "outputId": "47661b75-319f-4093-9d18-0ed49152fe24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of NaN values in X_train:  0\n",
            "Number of NaN values in X_test:  0\n",
            "Number of NaN values in y_train:  0\n",
            "Number of NaN values in y_test:  0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Number of NaN values in X_train: \", np.isnan(X_train_coffee).sum())\n",
        "print(\"Number of NaN values in X_test: \", np.isnan(X_test_coffee).sum())\n",
        "print(\"Number of NaN values in y_train: \", np.isnan(y_train_coffee).sum())\n",
        "print(\"Number of NaN values in y_test: \", np.isnan(y_test_coffee).sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzoP5e32iG2d"
      },
      "outputs": [],
      "source": [
        "mean = X_train_coffee.mean()\n",
        "std = X_train_coffee.std()\n",
        "\n",
        "X_train_normalized = (X_train_coffee- mean) / std\n",
        "X_test_normalized = (X_test_coffee - mean) / std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3LrGvvriTNz",
        "outputId": "91f8d9d5-e501-4afd-9dff-712a6a122bf9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((28, 1, 286), (28, 1, 286))"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_normalized.shape, X_test_normalized.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdVrMl0PiY1a",
        "outputId": "5d5bfc4c-2ef2-4bed-aa50-31dc2e9a4f9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.unique(y_train_coffee)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRUYhDwviqlS"
      },
      "outputs": [],
      "source": [
        "# Reshape the data to (samples, timesteps, features)\n",
        "X_train_coffee= X_train_normalized.reshape((-1, 286, 1))\n",
        "X_test_coffee = X_test_normalized.reshape((-1,286, 1))\n",
        "\n",
        "train_dataset_coffee = prepare_dataset(X_train_coffee, y_train_coffee, batch_size=batch_size)\n",
        "test_dataset_coffee = prepare_dataset(X_test_coffee, y_test_coffee, batch_size=batch_size)\n",
        "\n",
        "input_shape_3d = (286, 1)  # 286 timesteps, 1 feature\n",
        "\n",
        "num_classes = len(np.unique(y_train_coffee))\n",
        "fcn_model = create_fcn_model(input_shape_3d, num_classes)\n",
        "rnn_model = create_rnn_model(input_shape_3d, num_classes)\n",
        "cnn_model = create_cnn_model(input_shape_3d, num_classes)\n",
        "cnn_gru_model = create_cnn_gru_model(input_shape_3d, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rPllHk7jGdK",
        "outputId": "a2aecc47-36de-447e-9b4a-f914a686e992"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6907 - accuracy: 0.5357\n",
            "Epoch 1: val_loss improved from inf to 0.69170, saving model to best_model.h5\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6907 - accuracy: 0.5357 - val_loss: 0.6917 - val_accuracy: 0.4643\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6868 - accuracy: 0.5357\n",
            "Epoch 2: val_loss improved from 0.69170 to 0.68892, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.6868 - accuracy: 0.5357 - val_loss: 0.6889 - val_accuracy: 0.4643\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6833 - accuracy: 0.5714\n",
            "Epoch 3: val_loss improved from 0.68892 to 0.68637, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.6833 - accuracy: 0.5714 - val_loss: 0.6864 - val_accuracy: 0.5000\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6800 - accuracy: 0.5714\n",
            "Epoch 4: val_loss improved from 0.68637 to 0.68410, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.6800 - accuracy: 0.5714 - val_loss: 0.6841 - val_accuracy: 0.5357\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6770 - accuracy: 0.5714\n",
            "Epoch 5: val_loss improved from 0.68410 to 0.68206, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.6770 - accuracy: 0.5714 - val_loss: 0.6821 - val_accuracy: 0.5357\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6743 - accuracy: 0.6429\n",
            "Epoch 6: val_loss improved from 0.68206 to 0.68003, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.6743 - accuracy: 0.6429 - val_loss: 0.6800 - val_accuracy: 0.5714\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6720 - accuracy: 0.6429\n",
            "Epoch 7: val_loss improved from 0.68003 to 0.67804, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.6720 - accuracy: 0.6429 - val_loss: 0.6780 - val_accuracy: 0.5714\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6700 - accuracy: 0.6429\n",
            "Epoch 8: val_loss improved from 0.67804 to 0.67617, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.6700 - accuracy: 0.6429 - val_loss: 0.6762 - val_accuracy: 0.5714\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6681 - accuracy: 0.6429\n",
            "Epoch 9: val_loss improved from 0.67617 to 0.67427, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.6681 - accuracy: 0.6429 - val_loss: 0.6743 - val_accuracy: 0.5714\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6662 - accuracy: 0.6429\n",
            "Epoch 10: val_loss improved from 0.67427 to 0.67236, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.6662 - accuracy: 0.6429 - val_loss: 0.6724 - val_accuracy: 0.6071\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6724 - accuracy: 0.6071\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate each model\n",
        "fcn_history, fcn_test_loss, fcn_accuracy = train_model(fcn_model, train_dataset_coffee,test_dataset_coffee, epochs=10, learning_rate=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlmLigAGjNJq",
        "outputId": "c11aaa7e-31cd-4442-edc9-b1c9985a959f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7657 - accuracy: 0.5000\n",
            "Epoch 1: val_loss improved from inf to 0.79108, saving model to best_model.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.7657 - accuracy: 0.5000 - val_loss: 0.7911 - val_accuracy: 0.4643\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7645 - accuracy: 0.5000\n",
            "Epoch 2: val_loss improved from 0.79108 to 0.78962, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.7645 - accuracy: 0.5000 - val_loss: 0.7896 - val_accuracy: 0.4643\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7633 - accuracy: 0.5000\n",
            "Epoch 3: val_loss improved from 0.78962 to 0.78817, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.7633 - accuracy: 0.5000 - val_loss: 0.7882 - val_accuracy: 0.4643\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7620 - accuracy: 0.5000\n",
            "Epoch 4: val_loss improved from 0.78817 to 0.78673, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.7620 - accuracy: 0.5000 - val_loss: 0.7867 - val_accuracy: 0.4643\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7608 - accuracy: 0.5000\n",
            "Epoch 5: val_loss improved from 0.78673 to 0.78530, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.7608 - accuracy: 0.5000 - val_loss: 0.7853 - val_accuracy: 0.4643\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7597 - accuracy: 0.5000\n",
            "Epoch 6: val_loss improved from 0.78530 to 0.78389, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.7597 - accuracy: 0.5000 - val_loss: 0.7839 - val_accuracy: 0.4643\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7585 - accuracy: 0.5000\n",
            "Epoch 7: val_loss improved from 0.78389 to 0.78249, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.7585 - accuracy: 0.5000 - val_loss: 0.7825 - val_accuracy: 0.4643\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7573 - accuracy: 0.5000\n",
            "Epoch 8: val_loss improved from 0.78249 to 0.78110, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.7573 - accuracy: 0.5000 - val_loss: 0.7811 - val_accuracy: 0.4643\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7562 - accuracy: 0.5000\n",
            "Epoch 9: val_loss improved from 0.78110 to 0.77972, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.7562 - accuracy: 0.5000 - val_loss: 0.7797 - val_accuracy: 0.4643\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7550 - accuracy: 0.5000\n",
            "Epoch 10: val_loss improved from 0.77972 to 0.77836, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.7550 - accuracy: 0.5000 - val_loss: 0.7784 - val_accuracy: 0.4643\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7784 - accuracy: 0.4643\n"
          ]
        }
      ],
      "source": [
        "rnn_history, rnn_test_loss, rnn_accuracy = train_model(rnn_model, train_dataset_coffee, test_dataset_coffee, epochs=10, learning_rate=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qwklxFFjXQR",
        "outputId": "8b31ccb1-1a05-4d01-f491-75ebf031e622"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7833 - accuracy: 0.5000\n",
            "Epoch 1: val_loss improved from inf to 0.74963, saving model to best_model.h5\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7833 - accuracy: 0.5000 - val_loss: 0.7496 - val_accuracy: 0.5357\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7806 - accuracy: 0.5000\n",
            "Epoch 2: val_loss improved from 0.74963 to 0.74748, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.7806 - accuracy: 0.5000 - val_loss: 0.7475 - val_accuracy: 0.5357\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7779 - accuracy: 0.5000\n",
            "Epoch 3: val_loss improved from 0.74748 to 0.74536, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.7779 - accuracy: 0.5000 - val_loss: 0.7454 - val_accuracy: 0.5357\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7752 - accuracy: 0.5000\n",
            "Epoch 4: val_loss improved from 0.74536 to 0.74327, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.7752 - accuracy: 0.5000 - val_loss: 0.7433 - val_accuracy: 0.5357\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7726 - accuracy: 0.5000\n",
            "Epoch 5: val_loss improved from 0.74327 to 0.74123, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.7726 - accuracy: 0.5000 - val_loss: 0.7412 - val_accuracy: 0.5357\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7700 - accuracy: 0.5000\n",
            "Epoch 6: val_loss improved from 0.74123 to 0.73923, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.7700 - accuracy: 0.5000 - val_loss: 0.7392 - val_accuracy: 0.5357\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7675 - accuracy: 0.5000\n",
            "Epoch 7: val_loss improved from 0.73923 to 0.73731, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.7675 - accuracy: 0.5000 - val_loss: 0.7373 - val_accuracy: 0.5357\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7650 - accuracy: 0.5000\n",
            "Epoch 8: val_loss improved from 0.73731 to 0.73544, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.7650 - accuracy: 0.5000 - val_loss: 0.7354 - val_accuracy: 0.5357\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7626 - accuracy: 0.5000\n",
            "Epoch 9: val_loss improved from 0.73544 to 0.73360, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.7626 - accuracy: 0.5000 - val_loss: 0.7336 - val_accuracy: 0.5357\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7601 - accuracy: 0.5000\n",
            "Epoch 10: val_loss improved from 0.73360 to 0.73180, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.7601 - accuracy: 0.5000 - val_loss: 0.7318 - val_accuracy: 0.5357\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.7318 - accuracy: 0.5357\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    cnn_history, cnn_test_loss, cnn_accuracy = train_model(cnn_model, train_dataset_coffee, test_dataset_coffee, epochs=10, learning_rate=learning_rate)\n",
        "except Exception as e:\n",
        "    print(\"An error occurred during training:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ro0Ds_cjYuZ",
        "outputId": "938840b9-b1d6-4f4c-9724-e0dc6c41c25a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7162 - accuracy: 0.5000\n",
            "Epoch 1: val_loss improved from inf to 0.72957, saving model to best_model.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.7162 - accuracy: 0.5000 - val_loss: 0.7296 - val_accuracy: 0.4643\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7159 - accuracy: 0.5000\n",
            "Epoch 2: val_loss improved from 0.72957 to 0.72910, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 382ms/step - loss: 0.7159 - accuracy: 0.5000 - val_loss: 0.7291 - val_accuracy: 0.4643\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7155 - accuracy: 0.5000\n",
            "Epoch 3: val_loss improved from 0.72910 to 0.72863, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 0.7155 - accuracy: 0.5000 - val_loss: 0.7286 - val_accuracy: 0.4643\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7152 - accuracy: 0.5000\n",
            "Epoch 4: val_loss improved from 0.72863 to 0.72817, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 0.7152 - accuracy: 0.5000 - val_loss: 0.7282 - val_accuracy: 0.4643\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7148 - accuracy: 0.5000\n",
            "Epoch 5: val_loss improved from 0.72817 to 0.72771, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.7148 - accuracy: 0.5000 - val_loss: 0.7277 - val_accuracy: 0.4643\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7145 - accuracy: 0.5000\n",
            "Epoch 6: val_loss improved from 0.72771 to 0.72726, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.7145 - accuracy: 0.5000 - val_loss: 0.7273 - val_accuracy: 0.4643\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7141 - accuracy: 0.5000\n",
            "Epoch 7: val_loss improved from 0.72726 to 0.72681, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.7141 - accuracy: 0.5000 - val_loss: 0.7268 - val_accuracy: 0.4643\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7138 - accuracy: 0.5000\n",
            "Epoch 8: val_loss improved from 0.72681 to 0.72636, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.7138 - accuracy: 0.5000 - val_loss: 0.7264 - val_accuracy: 0.4643\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7135 - accuracy: 0.5000\n",
            "Epoch 9: val_loss improved from 0.72636 to 0.72592, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.7135 - accuracy: 0.5000 - val_loss: 0.7259 - val_accuracy: 0.4643\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7131 - accuracy: 0.5000\n",
            "Epoch 10: val_loss improved from 0.72592 to 0.72548, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.7131 - accuracy: 0.5000 - val_loss: 0.7255 - val_accuracy: 0.4643\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.7255 - accuracy: 0.4643\n"
          ]
        }
      ],
      "source": [
        "cnn_gru_history, cnn_gru_test_loss, cnn_gru_accuracy = train_model(cnn_gru_model, train_dataset_coffee, test_dataset_coffee, epochs=10, learning_rate=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYQQDZ45jgra",
        "outputId": "6786f736-b9fa-4fb0-bc94-a8629aa27c64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fully Connected Network Test Accuracy: 0.6071\n",
            "One Directional RNN Test Accuracy: 0.4643\n",
            "1D-CNN Test Accuracy: 0.5357\n",
            "1D-CNN + GRU Test Accuracy: 0.4643\n"
          ]
        }
      ],
      "source": [
        "fcn_test_loss, fcn_test_accuracy = fcn_model.evaluate(X_test_coffee, y_test_coffee, verbose=0)\n",
        "print(f\"Fully Connected Network Test Accuracy: {fcn_test_accuracy:.4f}\")\n",
        "\n",
        "rnn_test_loss, rnn_test_accuracy = rnn_model.evaluate(X_test_coffee, y_test_coffee, verbose=0)\n",
        "print(f\"One Directional RNN Test Accuracy: {rnn_test_accuracy:.4f}\")\n",
        "\n",
        "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(X_test_coffee, y_test_coffee, verbose=0)\n",
        "print(f\"1D-CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
        "\n",
        "cnn_gru_test_loss, cnn_gru_test_accuracy = cnn_gru_model.evaluate(X_test_coffee, y_test_coffee, verbose=0)\n",
        "print(f\"1D-CNN + GRU Test Accuracy: {cnn_gru_test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WVG4IjDj_hJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train_wafer, y_train_wafer = load_UCR_UEA_dataset(\"Wafer\", split=\"train\", return_X_y=True,return_type=\"numpy3D\")\n",
        "X_test_wafer, y_test_wafer = load_UCR_UEA_dataset(\"Wafer\", split=\"test\", return_X_y=True,return_type=\"numpy3D\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDCxCzpakl06"
      },
      "outputs": [],
      "source": [
        "if not isinstance(y_train_wafer, np.ndarray):\n",
        "    y_train_wafer = np.array(y_train_wafer)\n",
        "if not isinstance(y_test_wafer, np.ndarray):\n",
        "    y_test_itay_train_wafer = np.array(y_test_wafer)\n",
        "\n",
        "y_train_wafer = y_train_wafer.astype('int64')\n",
        "y_test_wafer = y_test_wafer.astype('int64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcsgXpS_k5h_",
        "outputId": "219ec114-c6f2-47fe-e44c-a8acca783331"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of NaN values in X_train:  0\n",
            "Number of NaN values in X_test:  0\n",
            "Number of NaN values in y_train:  0\n",
            "Number of NaN values in y_test:  0\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of NaN values in X_train: \", np.isnan(X_train_wafer).sum())\n",
        "print(\"Number of NaN values in X_test: \", np.isnan(X_test_wafer).sum())\n",
        "print(\"Number of NaN values in y_train: \", np.isnan(y_train_wafer).sum())\n",
        "print(\"Number of NaN values in y_test: \", np.isnan(y_test_wafer).sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUMYL8U5mebv"
      },
      "outputs": [],
      "source": [
        "mean = X_train_wafer.mean()\n",
        "std = X_train_wafer.std()\n",
        "\n",
        "X_train_normalized = (X_train_wafer- mean) / std\n",
        "X_test_normalized = (X_test_wafer - mean) / std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnZ3W4fpncQp",
        "outputId": "fc7d3351-47ad-401f-f406-e4b691001281"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1000, 1, 152), (6164, 1, 152))"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_normalized.shape, X_test_normalized.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yQJYk9anfrA",
        "outputId": "710a3b1e-a1dc-4344-fc0d-81096ad730ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-1,  1])"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.unique(y_train_wafer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENLDEVRjnmmi"
      },
      "outputs": [],
      "source": [
        "y_train_wafer[y_train_wafer == -1] = 0\n",
        "y_test_wafer[y_test_wafer == -1] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Je4GxKponrM"
      },
      "outputs": [],
      "source": [
        "# Reshape the data to (samples, timesteps, features)\n",
        "X_train_wafer= X_train_normalized.reshape((-1, 152, 1))\n",
        "X_test_wafer = X_test_normalized.reshape((-1,152, 1))\n",
        "\n",
        "train_dataset_wafer = prepare_dataset(X_train_wafer, y_train_wafer, batch_size=batch_size)\n",
        "test_dataset_wafer = prepare_dataset(X_test_wafer, y_test_wafer, batch_size=batch_size)\n",
        "\n",
        "input_shape_3d = (152, 1)  # 286 timesteps, 1 feature\n",
        "\n",
        "num_classes = len(np.unique(y_train_wafer))\n",
        "fcn_model = create_fcn_model(input_shape_3d, num_classes)\n",
        "rnn_model = create_rnn_model(input_shape_3d, num_classes)\n",
        "cnn_model = create_cnn_model(input_shape_3d, num_classes)\n",
        "cnn_gru_model = create_cnn_gru_model(input_shape_3d, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnZeAAGjpDkm",
        "outputId": "07b332e1-367f-43bb-8a64-2a99428e3dba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "30/32 [===========================>..] - ETA: 0s - loss: 0.7309 - accuracy: 0.3938\n",
            "Epoch 1: val_loss improved from inf to 0.66632, saving model to best_model.h5\n",
            "32/32 [==============================] - 2s 29ms/step - loss: 0.7285 - accuracy: 0.4050 - val_loss: 0.6663 - val_accuracy: 0.6650\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10\n",
            "28/32 [=========================>....] - ETA: 0s - loss: 0.6244 - accuracy: 0.7232\n",
            "Epoch 2: val_loss improved from 0.66632 to 0.58693, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 30ms/step - loss: 0.6208 - accuracy: 0.7320 - val_loss: 0.5869 - val_accuracy: 0.7967\n",
            "Epoch 3/10\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 0.5489 - accuracy: 0.8869\n",
            "Epoch 3: val_loss improved from 0.58693 to 0.51642, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 34ms/step - loss: 0.5473 - accuracy: 0.8850 - val_loss: 0.5164 - val_accuracy: 0.9031\n",
            "Epoch 4/10\n",
            "28/32 [=========================>....] - ETA: 0s - loss: 0.4690 - accuracy: 0.9219\n",
            "Epoch 4: val_loss improved from 0.51642 to 0.43556, saving model to best_model.h5\n",
            "32/32 [==============================] - 3s 96ms/step - loss: 0.4654 - accuracy: 0.9200 - val_loss: 0.4356 - val_accuracy: 0.9114\n",
            "Epoch 5/10\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 0.3928 - accuracy: 0.9256\n",
            "Epoch 5: val_loss improved from 0.43556 to 0.37683, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.3915 - accuracy: 0.9260 - val_loss: 0.3768 - val_accuracy: 0.9126\n",
            "Epoch 6/10\n",
            "25/32 [======================>.......] - ETA: 0s - loss: 0.3369 - accuracy: 0.9237\n",
            "Epoch 6: val_loss improved from 0.37683 to 0.32771, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.3359 - accuracy: 0.9270 - val_loss: 0.3277 - val_accuracy: 0.9194\n",
            "Epoch 7/10\n",
            "28/32 [=========================>....] - ETA: 0s - loss: 0.2931 - accuracy: 0.9364\n",
            "Epoch 7: val_loss improved from 0.32771 to 0.28510, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 29ms/step - loss: 0.2894 - accuracy: 0.9380 - val_loss: 0.2851 - val_accuracy: 0.9263\n",
            "Epoch 8/10\n",
            "27/32 [========================>.....] - ETA: 0s - loss: 0.2531 - accuracy: 0.9433\n",
            "Epoch 8: val_loss improved from 0.28510 to 0.25056, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 30ms/step - loss: 0.2510 - accuracy: 0.9430 - val_loss: 0.2506 - val_accuracy: 0.9345\n",
            "Epoch 9/10\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 0.2211 - accuracy: 0.9461\n",
            "Epoch 9: val_loss improved from 0.25056 to 0.22189, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 30ms/step - loss: 0.2198 - accuracy: 0.9470 - val_loss: 0.2219 - val_accuracy: 0.9411\n",
            "Epoch 10/10\n",
            "27/32 [========================>.....] - ETA: 0s - loss: 0.1920 - accuracy: 0.9549\n",
            "Epoch 10: val_loss improved from 0.22189 to 0.19671, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 24ms/step - loss: 0.1936 - accuracy: 0.9540 - val_loss: 0.1967 - val_accuracy: 0.9486\n",
            "193/193 [==============================] - 1s 3ms/step - loss: 0.1967 - accuracy: 0.9486\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate each model\n",
        "fcn_history, fcn_test_loss, fcn_accuracy = train_model(fcn_model, train_dataset_wafer, test_dataset_wafer, epochs=10, learning_rate=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftvODCmDpNhh",
        "outputId": "a0cfb1e0-9b8d-4db7-f737-4010baff5e78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.6486 - accuracy: 0.6810\n",
            "Epoch 1: val_loss improved from inf to 0.65152, saving model to best_model.h5\n",
            "32/32 [==============================] - 10s 196ms/step - loss: 0.6486 - accuracy: 0.6810 - val_loss: 0.6515 - val_accuracy: 0.6535\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.6326 - accuracy: 0.6790\n",
            "Epoch 2: val_loss improved from 0.65152 to 0.64249, saving model to best_model.h5\n",
            "32/32 [==============================] - 13s 422ms/step - loss: 0.6326 - accuracy: 0.6790 - val_loss: 0.6425 - val_accuracy: 0.6549\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.6222 - accuracy: 0.6820\n",
            "Epoch 3: val_loss improved from 0.64249 to 0.63621, saving model to best_model.h5\n",
            "32/32 [==============================] - 8s 262ms/step - loss: 0.6222 - accuracy: 0.6820 - val_loss: 0.6362 - val_accuracy: 0.6570\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.6145 - accuracy: 0.6880\n",
            "Epoch 4: val_loss improved from 0.63621 to 0.63068, saving model to best_model.h5\n",
            "32/32 [==============================] - 7s 236ms/step - loss: 0.6145 - accuracy: 0.6880 - val_loss: 0.6307 - val_accuracy: 0.6619\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.6074 - accuracy: 0.6900\n",
            "Epoch 5: val_loss improved from 0.63068 to 0.62483, saving model to best_model.h5\n",
            "32/32 [==============================] - 7s 237ms/step - loss: 0.6074 - accuracy: 0.6900 - val_loss: 0.6248 - val_accuracy: 0.6660\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.6001 - accuracy: 0.6970\n",
            "Epoch 6: val_loss improved from 0.62483 to 0.61790, saving model to best_model.h5\n",
            "32/32 [==============================] - 6s 189ms/step - loss: 0.6001 - accuracy: 0.6970 - val_loss: 0.6179 - val_accuracy: 0.6690\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.5916 - accuracy: 0.7000\n",
            "Epoch 7: val_loss improved from 0.61790 to 0.60876, saving model to best_model.h5\n",
            "32/32 [==============================] - 7s 235ms/step - loss: 0.5916 - accuracy: 0.7000 - val_loss: 0.6088 - val_accuracy: 0.6728\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.5816 - accuracy: 0.7030\n",
            "Epoch 8: val_loss improved from 0.60876 to 0.59723, saving model to best_model.h5\n",
            "32/32 [==============================] - 8s 240ms/step - loss: 0.5816 - accuracy: 0.7030 - val_loss: 0.5972 - val_accuracy: 0.6755\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.5704 - accuracy: 0.7070\n",
            "Epoch 9: val_loss improved from 0.59723 to 0.58456, saving model to best_model.h5\n",
            "32/32 [==============================] - 7s 222ms/step - loss: 0.5704 - accuracy: 0.7070 - val_loss: 0.5846 - val_accuracy: 0.6773\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.5582 - accuracy: 0.7090\n",
            "Epoch 10: val_loss improved from 0.58456 to 0.57225, saving model to best_model.h5\n",
            "32/32 [==============================] - 15s 480ms/step - loss: 0.5582 - accuracy: 0.7090 - val_loss: 0.5722 - val_accuracy: 0.6799\n",
            "193/193 [==============================] - 4s 21ms/step - loss: 0.5722 - accuracy: 0.6799\n"
          ]
        }
      ],
      "source": [
        "rnn_history, rnn_test_loss, rnn_accuracy = train_model(rnn_model, train_dataset_wafer, test_dataset_wafer, epochs=10, learning_rate=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUeaXSrqqPKg",
        "outputId": "79882c02-c8c5-47c3-8001-73e3be2ec03a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "22/32 [===================>..........] - ETA: 0s - loss: 0.7477 - accuracy: 0.3366\n",
            "Epoch 1: val_loss improved from inf to 0.66890, saving model to best_model.h5\n",
            "32/32 [==============================] - 2s 31ms/step - loss: 0.7308 - accuracy: 0.4040 - val_loss: 0.6689 - val_accuracy: 0.6531\n",
            "Epoch 2/10\n",
            "26/32 [=======================>......] - ETA: 0s - loss: 0.6398 - accuracy: 0.7572\n",
            "Epoch 2: val_loss improved from 0.66890 to 0.58751, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 25ms/step - loss: 0.6351 - accuracy: 0.7670 - val_loss: 0.5875 - val_accuracy: 0.8650\n",
            "Epoch 3/10\n",
            "23/32 [====================>.........] - ETA: 0s - loss: 0.5676 - accuracy: 0.8777\n",
            "Epoch 3: val_loss improved from 0.58751 to 0.52310, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 26ms/step - loss: 0.5591 - accuracy: 0.8760 - val_loss: 0.5231 - val_accuracy: 0.8787\n",
            "Epoch 4/10\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.4994 - accuracy: 0.8770\n",
            "Epoch 4: val_loss improved from 0.52310 to 0.47351, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 28ms/step - loss: 0.4992 - accuracy: 0.8770 - val_loss: 0.4735 - val_accuracy: 0.8726\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.4519 - accuracy: 0.8810\n",
            "Epoch 5: val_loss improved from 0.47351 to 0.43437, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 26ms/step - loss: 0.4519 - accuracy: 0.8810 - val_loss: 0.4344 - val_accuracy: 0.8780\n",
            "Epoch 6/10\n",
            "27/32 [========================>.....] - ETA: 0s - loss: 0.4172 - accuracy: 0.8889\n",
            "Epoch 6: val_loss improved from 0.43437 to 0.40348, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 0.4146 - accuracy: 0.8910 - val_loss: 0.4035 - val_accuracy: 0.8834\n",
            "Epoch 7/10\n",
            "27/32 [========================>.....] - ETA: 0s - loss: 0.3893 - accuracy: 0.8947\n",
            "Epoch 7: val_loss improved from 0.40348 to 0.37972, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 26ms/step - loss: 0.3858 - accuracy: 0.8970 - val_loss: 0.3797 - val_accuracy: 0.8876\n",
            "Epoch 8/10\n",
            "25/32 [======================>.......] - ETA: 0s - loss: 0.3644 - accuracy: 0.8975\n",
            "Epoch 8: val_loss improved from 0.37972 to 0.36072, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 0.3628 - accuracy: 0.8990 - val_loss: 0.3607 - val_accuracy: 0.8897\n",
            "Epoch 9/10\n",
            "23/32 [====================>.........] - ETA: 0s - loss: 0.3471 - accuracy: 0.8981\n",
            "Epoch 9: val_loss improved from 0.36072 to 0.34558, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 26ms/step - loss: 0.3445 - accuracy: 0.9020 - val_loss: 0.3456 - val_accuracy: 0.8910\n",
            "Epoch 10/10\n",
            "27/32 [========================>.....] - ETA: 0s - loss: 0.3284 - accuracy: 0.9039\n",
            "Epoch 10: val_loss improved from 0.34558 to 0.33343, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 26ms/step - loss: 0.3298 - accuracy: 0.9040 - val_loss: 0.3334 - val_accuracy: 0.8913\n",
            "193/193 [==============================] - 0s 2ms/step - loss: 0.3334 - accuracy: 0.8913\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    cnn_history, cnn_test_loss, cnn_accuracy = train_model(cnn_model, train_dataset_wafer, test_dataset_wafer, epochs=10, learning_rate=learning_rate)\n",
        "except Exception as e:\n",
        "    print(\"An error occurred during training:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoAgFt-uqfoC",
        "outputId": "665c2fa1-f41a-4b4f-b554-e4da6506102d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.6859 - accuracy: 0.6620\n",
            "Epoch 1: val_loss improved from inf to 0.67667, saving model to best_model.h5\n",
            "32/32 [==============================] - 11s 273ms/step - loss: 0.6859 - accuracy: 0.6620 - val_loss: 0.6767 - val_accuracy: 0.6639\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.6661 - accuracy: 0.8420\n",
            "Epoch 2: val_loss improved from 0.67667 to 0.65786, saving model to best_model.h5\n",
            "32/32 [==============================] - 13s 425ms/step - loss: 0.6661 - accuracy: 0.8420 - val_loss: 0.6579 - val_accuracy: 0.8921\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.6476 - accuracy: 0.9030\n",
            "Epoch 3: val_loss improved from 0.65786 to 0.64057, saving model to best_model.h5\n",
            "32/32 [==============================] - 13s 421ms/step - loss: 0.6476 - accuracy: 0.9030 - val_loss: 0.6406 - val_accuracy: 0.8921\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.6302 - accuracy: 0.9030\n",
            "Epoch 4: val_loss improved from 0.64057 to 0.62399, saving model to best_model.h5\n",
            "32/32 [==============================] - 8s 258ms/step - loss: 0.6302 - accuracy: 0.9030 - val_loss: 0.6240 - val_accuracy: 0.8921\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.6138 - accuracy: 0.9030\n",
            "Epoch 5: val_loss improved from 0.62399 to 0.60835, saving model to best_model.h5\n",
            "32/32 [==============================] - 8s 255ms/step - loss: 0.6138 - accuracy: 0.9030 - val_loss: 0.6083 - val_accuracy: 0.8921\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.5979 - accuracy: 0.9030\n",
            "Epoch 6: val_loss improved from 0.60835 to 0.59314, saving model to best_model.h5\n",
            "32/32 [==============================] - 8s 255ms/step - loss: 0.5979 - accuracy: 0.9030 - val_loss: 0.5931 - val_accuracy: 0.8921\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.5824 - accuracy: 0.9030\n",
            "Epoch 7: val_loss improved from 0.59314 to 0.57830, saving model to best_model.h5\n",
            "32/32 [==============================] - 9s 292ms/step - loss: 0.5824 - accuracy: 0.9030 - val_loss: 0.5783 - val_accuracy: 0.8921\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.5673 - accuracy: 0.9030\n",
            "Epoch 8: val_loss improved from 0.57830 to 0.56357, saving model to best_model.h5\n",
            "32/32 [==============================] - 13s 421ms/step - loss: 0.5673 - accuracy: 0.9030 - val_loss: 0.5636 - val_accuracy: 0.8921\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.5525 - accuracy: 0.9030\n",
            "Epoch 9: val_loss improved from 0.56357 to 0.54939, saving model to best_model.h5\n",
            "32/32 [==============================] - 8s 255ms/step - loss: 0.5525 - accuracy: 0.9030 - val_loss: 0.5494 - val_accuracy: 0.8921\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.5379 - accuracy: 0.9030\n",
            "Epoch 10: val_loss improved from 0.54939 to 0.53528, saving model to best_model.h5\n",
            "32/32 [==============================] - 9s 293ms/step - loss: 0.5379 - accuracy: 0.9030 - val_loss: 0.5353 - val_accuracy: 0.8921\n",
            "193/193 [==============================] - 5s 27ms/step - loss: 0.5353 - accuracy: 0.8921\n"
          ]
        }
      ],
      "source": [
        "cnn_gru_history, cnn_gru_test_loss, cnn_gru_accuracy = train_model(cnn_gru_model, train_dataset_wafer, test_dataset_wafer, epochs=10, learning_rate=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeituI-hqk9l",
        "outputId": "1002cd04-d9ac-4827-e61a-e0f5c9af9462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fully Connected Network Test Accuracy: 0.9486\n",
            "One Directional RNN Test Accuracy: 0.6799\n",
            "1D-CNN Test Accuracy: 0.8913\n",
            "1D-CNN + GRU Test Accuracy: 0.8921\n"
          ]
        }
      ],
      "source": [
        "fcn_test_loss, fcn_test_accuracy = fcn_model.evaluate(X_test_wafer, y_test_wafer, verbose=0)\n",
        "print(f\"Fully Connected Network Test Accuracy: {fcn_test_accuracy:.4f}\")\n",
        "\n",
        "rnn_test_loss, rnn_test_accuracy = rnn_model.evaluate(X_test_wafer, y_test_wafer, verbose=0)\n",
        "print(f\"One Directional RNN Test Accuracy: {rnn_test_accuracy:.4f}\")\n",
        "\n",
        "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(X_test_wafer, y_test_wafer, verbose=0)\n",
        "print(f\"1D-CNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
        "\n",
        "cnn_gru_test_loss, cnn_gru_test_accuracy = cnn_gru_model.evaluate(X_test_wafer, y_test_wafer, verbose=0)\n",
        "print(f\"1D-CNN + GRU Test Accuracy: {cnn_gru_test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEevYhlXFWge"
      },
      "source": [
        "### Task 2: Time series classification using deep learning 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm1wqK6EGs9H"
      },
      "source": [
        "There has been several neural network models dedicated to time series classification. Besides your own models that you developed in Lab 4, now you will develop such dedicated models by referring to some papers, and test if they indeed perform better than your rough models. There are two famous papers as follows:\n",
        " - [Convolutional neural networks for time series classification (2017)](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7870510)\n",
        " - [Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline (2017)](https://arxiv.org/abs/1611.06455)\n",
        "\n",
        "First paper's idea is already implemented in sktime, with the name `CNNClassifier`. Second paper has three models and those are easy to develop using tensorflow. Now the task is to develop two models (MLP and FCN) in the second paper and test it together with `CNNClassifier`.\n",
        "\n",
        "Use the same four datasets, and test sktime's `CNNClassifier` and MLP and FCN models you develop. Report test scores of three models (`CNNClassifier`, MLP, and FCN) on three datasets you chose. It would be nine scores in total. For MLP and FCN, you may need to satisfy the following requirement:\n",
        "\n",
        "- You should use at least **two** Tensorflow callbacks when you fit your model. These can be built-in ones or your personalized callback. If you use Torch, explain how you implement the equivalent operations.\n",
        "- You should run the model at least 10 epochs.\n",
        "- You can use the same processed datasets in Task 1. For `CNNClassifier`, as you cannot use `tf.Data`, you may put the training set directly.\n",
        "- For `CNNClassifier`, you can run it with the default parameters or reduce the number of epoch (default is 2000).\n",
        "- Please use the predefined test dataset to report the test scores.\n",
        "\n",
        "Note that the main purpose of this task is to check if you can develop a similar network structure with description. If the detail of the specific part (e.g., size of one layer or some custom parameters like epoch) is missing in the paper, you can set it on your own.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_vMm1NzcXst"
      },
      "outputs": [],
      "source": [
        "from sktime.classification.deep_learning.cnn import CNNClassifier\n",
        "from sktime.datasets import load_unit_test\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, Conv1D, GRU, Flatten, Input,Dropout\n",
        "from tensorflow.keras.layers import MaxPooling1D\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMiGXexwerHx",
        "outputId": "5ad1f4a6-d33d-40ba-d67e-655f5957c5f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7/7 [==============================] - 0s 3ms/step\n",
            "CNNClassifier Test Accuracy: 0.82\n"
          ]
        }
      ],
      "source": [
        "#ecg200\n",
        "X_train_ecg200, y_train_ecg200 = load_UCR_UEA_dataset(name=\"ECG200\", split=\"train\", return_X_y=True, return_type=\"numpy3D\")\n",
        "X_test_ecg200, y_test_ecg200= load_UCR_UEA_dataset(name=\"ECG200\", split=\"test\", return_X_y=True, return_type=\"numpy3D\")\n",
        "\n",
        "cnn = CNNClassifier(n_epochs=200)\n",
        "cnn.fit(X_train_ecg200, y_train_ecg200)\n",
        "cnn_score = cnn.score(X_test_ecg200,y_test_ecg200)\n",
        "print(\"CNNClassifier Test Accuracy:\", cnn_score)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sp9Bi4cVAYs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4Gop64Yd72c",
        "outputId": "33e26754-1df8-40f3-b037-3d4d4cd908c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 8ms/step\n",
            "CNNClassifier Test Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "#coffee\n",
        "X_train_coffee, y_train_coffee = load_UCR_UEA_dataset(name=\"Coffee\", split=\"train\", return_X_y=True, return_type=\"numpy3D\")\n",
        "X_test_coffee, y_test_coffee= load_UCR_UEA_dataset(name=\"Coffee\", split=\"test\", return_X_y=True, return_type=\"numpy3D\")\n",
        "\n",
        "cnn = CNNClassifier(n_epochs=200)\n",
        "cnn.fit(X_train_coffee, y_train_coffee)\n",
        "cnn_score = cnn.score(X_test_coffee,y_test_coffee)\n",
        "print(\"CNNClassifier Test Accuracy:\", cnn_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BX_ULo3EU37",
        "outputId": "a809cd93-c4de-4959-958b-6ee9243f11a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "386/386 [==============================] - 2s 4ms/step\n",
            "CNNClassifier Test Accuracy: 0.9928617780661908\n"
          ]
        }
      ],
      "source": [
        "#wafer\n",
        "X_train_wafer, y_train_wafer = load_UCR_UEA_dataset(name=\"Wafer\", split=\"train\", return_X_y=True, return_type=\"numpy3D\")\n",
        "X_test_wafer, y_test_wafer= load_UCR_UEA_dataset(name=\"Wafer\", split=\"test\", return_X_y=True, return_type=\"numpy3D\")\n",
        "\n",
        "cnn = CNNClassifier(n_epochs=200)\n",
        "cnn.fit(X_train_wafer, y_train_wafer)\n",
        "cnn_score = cnn.score(X_test_wafer,y_test_wafer)\n",
        "print(\"CNNClassifier Test Accuracy:\", cnn_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT2czHRNuzyh"
      },
      "outputs": [],
      "source": [
        "def create_mlp_model(input_shape, num_classes):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=input_shape),\n",
        "        tf.keras.layers.Dense(500, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.1),\n",
        "        tf.keras.layers.Dense(500, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(500, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhgNhQaxvB-D"
      },
      "outputs": [],
      "source": [
        "def create_fcn_model(input_shape, num_classes):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=input_shape),\n",
        "        tf.keras.layers.Conv1D(filters=128, kernel_size=8, padding='same', activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Conv1D(filters=256, kernel_size=5, padding='same', activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9sBLIkAvFmm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "def train_and_evaluate_model(model, train_dataset, test_dataset, epochs=10, learning_rate=1e-5):\n",
        "    def check_for_nans(dataset):\n",
        "        for features, labels in dataset:\n",
        "            if tf.reduce_any(tf.math.is_nan(features)) or tf.reduce_any(tf.math.is_nan(labels)):\n",
        "                raise ValueError(\"NaN values detected in the dataset.\")\n",
        "\n",
        "    check_for_nans(train_dataset)\n",
        "    check_for_nans(test_dataset)\n",
        "\n",
        "    optimizer = Adam(learning_rate=learning_rate, clipvalue=0.5)\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=3, verbose=1),\n",
        "        ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        epochs=epochs,\n",
        "        validation_data=test_dataset,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "    try:\n",
        "        model.load_weights('best_model.h5')\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Checkpoint file not found: {e}. Using model as is after training.\")\n",
        "\n",
        "    test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "    return history, test_loss, test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcnQuvZBeJ8p"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(X, y, batch_size=32, shuffle_buffer_size=100):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    if shuffle_buffer_size > 0:\n",
        "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UceY2CuWx6Ly"
      },
      "outputs": [],
      "source": [
        "#ecg200 dataset\n",
        "\n",
        "X_train_ecg200, y_train_ecg200 = load_UCR_UEA_dataset(name=\"ECG200\", split=\"train\", return_X_y=True, return_type=\"numpy3D\")\n",
        "X_test_ecg200, y_test_ecg200 = load_UCR_UEA_dataset(name=\"ECG200\", split=\"test\", return_X_y=True, return_type=\"numpy3D\")\n",
        "\n",
        "if not isinstance(y_train_ecg200, np.ndarray):\n",
        "    y_train_ecg200 = np.array(y_train_ecg200)\n",
        "if not isinstance(y_test_ecg200, np.ndarray):\n",
        "    y_test_ecg200 = np.array(y_test_ecg200)\n",
        "\n",
        "y_train_ecg200 = y_train_ecg200.astype('int64')\n",
        "y_test_ecg200 = y_test_ecg200.astype('int64')\n",
        "\n",
        "mean = X_train_ecg200.mean()\n",
        "std = X_train_ecg200.std()\n",
        "\n",
        "X_train_ecg200 = (X_train_ecg200 - mean) / std\n",
        "X_test_ecg200 = (X_test_ecg200 - mean) / std\n",
        "\n",
        "y_train_ecg200[y_train_ecg200 == -1] = 0\n",
        "y_test_ecg200[y_test_ecg200 == -1] = 0\n",
        "\n",
        "X_train_ecg200 = X_train_ecg200.reshape((-1, 96, 1))\n",
        "X_test_ecg200 = X_test_ecg200.reshape((-1, 96, 1))\n",
        "\n",
        "train_dataset_ecg200 = prepare_dataset(X_train_ecg200, y_train_ecg200, batch_size=batch_size)\n",
        "test_dataset_ecg200 = prepare_dataset(X_test_ecg200, y_test_ecg200, batch_size=batch_size)\n",
        "num_classes = len(np.unique(y_train_ecg200))\n",
        "\n",
        "input_shape_3d = (96, 1)\n",
        "mlp_model = create_mlp_model(input_shape_3d, num_classes)\n",
        "fcn_model = create_fcn_model(input_shape_3d, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeMKyj6GeZ5x",
        "outputId": "fcb984a2-5cbd-49da-da62-9915c95a1a1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1/4 [======>.......................] - ETA: 3s - loss: 0.7929 - accuracy: 0.3125\n",
            "Epoch 1: val_loss improved from inf to 0.74581, saving model to best_model.h5\n",
            "4/4 [==============================] - 1s 85ms/step - loss: 0.7478 - accuracy: 0.4300 - val_loss: 0.7458 - val_accuracy: 0.2200\n",
            "Epoch 2/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.7407 - accuracy: 0.4375\n",
            "Epoch 2: val_loss improved from 0.74581 to 0.72618, saving model to best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4/4 [==============================] - 0s 37ms/step - loss: 0.7360 - accuracy: 0.4700 - val_loss: 0.7262 - val_accuracy: 0.2100\n",
            "Epoch 3/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.7015 - accuracy: 0.5000\n",
            "Epoch 3: val_loss improved from 0.72618 to 0.70889, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.7070 - accuracy: 0.5000 - val_loss: 0.7089 - val_accuracy: 0.3700\n",
            "Epoch 4/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6840 - accuracy: 0.5312\n",
            "Epoch 4: val_loss improved from 0.70889 to 0.69407, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.7024 - accuracy: 0.4800 - val_loss: 0.6941 - val_accuracy: 0.5700\n",
            "Epoch 5/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.7222 - accuracy: 0.4688\n",
            "Epoch 5: val_loss improved from 0.69407 to 0.68022, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.6770 - accuracy: 0.5400 - val_loss: 0.6802 - val_accuracy: 0.6600\n",
            "Epoch 6/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6918 - accuracy: 0.4688\n",
            "Epoch 6: val_loss improved from 0.68022 to 0.66740, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.6550 - accuracy: 0.5800 - val_loss: 0.6674 - val_accuracy: 0.6800\n",
            "Epoch 7/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6983 - accuracy: 0.4062\n",
            "Epoch 7: val_loss improved from 0.66740 to 0.65557, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.6651 - accuracy: 0.5400 - val_loss: 0.6556 - val_accuracy: 0.6700\n",
            "Epoch 8/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6121 - accuracy: 0.6875\n",
            "Epoch 8: val_loss improved from 0.65557 to 0.64409, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.6477 - accuracy: 0.6700 - val_loss: 0.6441 - val_accuracy: 0.6700\n",
            "Epoch 9/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6575 - accuracy: 0.5938\n",
            "Epoch 9: val_loss improved from 0.64409 to 0.63316, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.6036 - accuracy: 0.7200 - val_loss: 0.6332 - val_accuracy: 0.6600\n",
            "Epoch 10/10\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5976 - accuracy: 0.7500\n",
            "Epoch 10: val_loss improved from 0.63316 to 0.62264, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.6156 - accuracy: 0.7000 - val_loss: 0.6226 - val_accuracy: 0.6600\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6226 - accuracy: 0.6600\n"
          ]
        }
      ],
      "source": [
        "mlp_history, mlp_test_loss, mlp_accuracy = train_model(mlp_model, train_dataset_ecg200, test_dataset_ecg200, epochs=10, learning_rate=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6G3Enszj2Go8",
        "outputId": "2f6b7778-e6d8-4351-b64a-8b042daebaff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6750 - accuracy: 0.6354\n",
            "Epoch 1: val_loss improved from inf to 0.68706, saving model to best_model.h5\n",
            "4/4 [==============================] - 4s 365ms/step - loss: 0.6759 - accuracy: 0.6400 - val_loss: 0.6871 - val_accuracy: 0.6400\n",
            "Epoch 2/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6513 - accuracy: 0.7708\n",
            "Epoch 2: val_loss did not improve from 0.68706\n",
            "4/4 [==============================] - 1s 154ms/step - loss: 0.6556 - accuracy: 0.7500 - val_loss: 0.6901 - val_accuracy: 0.6400\n",
            "Epoch 3/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6457 - accuracy: 0.7396\n",
            "Epoch 3: val_loss did not improve from 0.68706\n",
            "4/4 [==============================] - 1s 152ms/step - loss: 0.6449 - accuracy: 0.7500 - val_loss: 0.6929 - val_accuracy: 0.4500\n",
            "Epoch 4/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6271 - accuracy: 0.7708\n",
            "Epoch 4: val_loss did not improve from 0.68706\n",
            "4/4 [==============================] - 1s 153ms/step - loss: 0.6328 - accuracy: 0.7500 - val_loss: 0.6956 - val_accuracy: 0.3600\n",
            "Epoch 4: early stopping\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.6871 - accuracy: 0.6400\n"
          ]
        }
      ],
      "source": [
        "fcn_history, fcn_test_loss, fcn_accuracy = train_model(fcn_model, train_dataset_ecg200, test_dataset_ecg200, epochs=10, learning_rate=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILvdSSJGdwMe",
        "outputId": "048408dd-2ee5-4d72-840b-a5b67dc8b5df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fully Connected Network Test Accuracy: 0.6400\n",
            "MLP Test Accuracy: 0.6600\n"
          ]
        }
      ],
      "source": [
        "fcn_test_loss, fcn_test_accuracy = fcn_model.evaluate(X_test_ecg200, y_test_ecg200, verbose=0)\n",
        "print(f\"Fully Connected Network Test Accuracy: {fcn_test_accuracy:.4f}\")\n",
        "\n",
        "mlp_test_loss, mlp_test_accuracy = mlp_model.evaluate(X_test_ecg200, y_test_ecg200, verbose=0)\n",
        "print(f\"MLP Test Accuracy: {mlp_test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tetF8CHbg6za"
      },
      "outputs": [],
      "source": [
        "#Coffee dataset\n",
        "#num_classes = len(np.unique(y_train_coffee))\n",
        "X_train_coffee, y_train_coffee = load_UCR_UEA_dataset(name=\"Coffee\", split=\"train\", return_X_y=True, return_type=\"numpy3D\")\n",
        "X_test_coffee, y_test_coffee = load_UCR_UEA_dataset(name=\"Coffee\", split=\"test\", return_X_y=True, return_type=\"numpy3D\")\n",
        "\n",
        "if not isinstance(y_train_coffee, np.ndarray):\n",
        "    y_train_coffee = np.array(y_train_coffee)\n",
        "if not isinstance(y_test_coffee, np.ndarray):\n",
        "    y_test_coffee = np.array(y_test_coffee)\n",
        "\n",
        "y_train_coffee = y_train_coffee.astype('int64')\n",
        "y_test_coffee = y_test_coffee.astype('int64')\n",
        "\n",
        "mean = X_train_coffee.mean()\n",
        "std = X_train_coffee.std()\n",
        "\n",
        "X_train_coffee = (X_train_coffee - mean) / std\n",
        "X_test_coffee = (X_test_coffee - mean) / std\n",
        "\n",
        "y_train_coffee[y_train_coffee == -1] = 0\n",
        "y_test_coffee[y_test_coffee == -1] = 0\n",
        "\n",
        "X_train_coffee = X_train_coffee.reshape((-1, 286, 1))\n",
        "X_test_coffee = X_test_coffee.reshape((-1, 286, 1))\n",
        "\n",
        "train_dataset_coffee = prepare_dataset(X_train_coffee, y_train_coffee, batch_size=batch_size)\n",
        "test_dataset_coffee = prepare_dataset(X_test_coffee, y_test_coffee, batch_size=batch_size)\n",
        "\n",
        "input_shape_3d = (286, 1)\n",
        "num_classes = len(np.unique(y_train_coffee))\n",
        "mlp_model = create_mlp_model(input_shape_3d, num_classes)\n",
        "fcn_model = create_fcn_model(input_shape_3d, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6B4h2f6hfW2",
        "outputId": "91807799-727e-4d2f-a4d7-c0484139222c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.6429\n",
            "Epoch 1: val_loss improved from inf to 0.67908, saving model to best_model.h5\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6931 - accuracy: 0.6429 - val_loss: 0.6791 - val_accuracy: 0.5357\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6867 - accuracy: 0.5000\n",
            "Epoch 2: val_loss improved from 0.67908 to 0.67873, saving model to best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 87ms/step - loss: 0.6867 - accuracy: 0.5000 - val_loss: 0.6787 - val_accuracy: 0.5714\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7167 - accuracy: 0.5000\n",
            "Epoch 3: val_loss improved from 0.67873 to 0.67842, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.7167 - accuracy: 0.5000 - val_loss: 0.6784 - val_accuracy: 0.5714\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6874 - accuracy: 0.6071\n",
            "Epoch 4: val_loss improved from 0.67842 to 0.67817, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.6874 - accuracy: 0.6071 - val_loss: 0.6782 - val_accuracy: 0.6071\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6637 - accuracy: 0.5357\n",
            "Epoch 5: val_loss improved from 0.67817 to 0.67783, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.6637 - accuracy: 0.5357 - val_loss: 0.6778 - val_accuracy: 0.7143\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6721 - accuracy: 0.5357\n",
            "Epoch 6: val_loss improved from 0.67783 to 0.67749, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.6721 - accuracy: 0.5357 - val_loss: 0.6775 - val_accuracy: 0.8929\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7171 - accuracy: 0.5000\n",
            "Epoch 7: val_loss improved from 0.67749 to 0.67709, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.7171 - accuracy: 0.5000 - val_loss: 0.6771 - val_accuracy: 0.8571\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6239 - accuracy: 0.6429\n",
            "Epoch 8: val_loss improved from 0.67709 to 0.67664, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.6239 - accuracy: 0.6429 - val_loss: 0.6766 - val_accuracy: 0.8929\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6579 - accuracy: 0.5714\n",
            "Epoch 9: val_loss improved from 0.67664 to 0.67611, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.6579 - accuracy: 0.5714 - val_loss: 0.6761 - val_accuracy: 0.9286\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7895 - accuracy: 0.5000\n",
            "Epoch 10: val_loss improved from 0.67611 to 0.67552, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.7895 - accuracy: 0.5000 - val_loss: 0.6755 - val_accuracy: 0.8929\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6755 - accuracy: 0.8929\n"
          ]
        }
      ],
      "source": [
        "mlp_history, mlp_test_loss, mlp_accuracy = train_model(mlp_model, train_dataset_coffee, test_dataset_coffee, epochs=10, learning_rate=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRq3c9x4h5uL",
        "outputId": "b3f89ec5-81b7-46cd-c861-f75f5d3cf020"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6793 - accuracy: 1.0000\n",
            "Epoch 1: val_loss improved from inf to 0.69507, saving model to best_model.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.6793 - accuracy: 1.0000 - val_loss: 0.6951 - val_accuracy: 0.4643\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6750 - accuracy: 1.0000\n",
            "Epoch 2: val_loss did not improve from 0.69507\n",
            "1/1 [==============================] - 1s 695ms/step - loss: 0.6750 - accuracy: 1.0000 - val_loss: 0.6951 - val_accuracy: 0.4643\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6706 - accuracy: 1.0000\n",
            "Epoch 3: val_loss did not improve from 0.69507\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 0.6706 - accuracy: 1.0000 - val_loss: 0.6951 - val_accuracy: 0.4643\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6663 - accuracy: 1.0000\n",
            "Epoch 4: val_loss did not improve from 0.69507\n",
            "1/1 [==============================] - 0s 422ms/step - loss: 0.6663 - accuracy: 1.0000 - val_loss: 0.6951 - val_accuracy: 0.4643\n",
            "Epoch 4: early stopping\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.6951 - accuracy: 0.4643\n"
          ]
        }
      ],
      "source": [
        "fcn_history, fcn_test_loss, fcn_accuracy = train_model(fcn_model, train_dataset_coffee, test_dataset_coffee, epochs=10, learning_rate=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thhRETJUiACS",
        "outputId": "73830ae9-b2e9-4459-e91b-736ce10d89dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fully Connected Network Test Accuracy: 0.4643\n",
            "MLP Test Accuracy: 0.8929\n"
          ]
        }
      ],
      "source": [
        "\n",
        "fcn_test_loss, fcn_test_accuracy = fcn_model.evaluate(X_test_coffee, y_test_coffee, verbose=0)\n",
        "print(f\"Fully Connected Network Test Accuracy: {fcn_test_accuracy:.4f}\")\n",
        "\n",
        "mlp_test_loss, mlp_test_accuracy = mlp_model.evaluate(X_test_coffee, y_test_coffee, verbose=0)\n",
        "print(f\"MLP Test Accuracy: {mlp_test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHs0_eJaidj-"
      },
      "outputs": [],
      "source": [
        "X_train_wafer, y_train_wafer = load_UCR_UEA_dataset(name=\"Wafer\", split=\"train\", return_X_y=True, return_type=\"numpy3D\")\n",
        "X_test_wafer, y_test_wafer = load_UCR_UEA_dataset(name=\"Wafer\", split=\"test\", return_X_y=True, return_type=\"numpy3D\")\n",
        "\n",
        "if not isinstance(y_train_wafer, np.ndarray):\n",
        "    y_train_wafer = np.array(y_train_wafer)\n",
        "if not isinstance(y_test_wafer, np.ndarray):\n",
        "    y_test_wafer = np.array(y_test_wafer)\n",
        "\n",
        "y_train_wafer = y_train_wafer.astype('int64')\n",
        "y_test_wafer = y_test_wafer.astype('int64')\n",
        "\n",
        "mean = X_train_wafer.mean()\n",
        "std = X_train_wafer.std()\n",
        "\n",
        "X_train_wafer = (X_train_wafer - mean) / std\n",
        "X_test_wafer = (X_test_wafer - mean) / std\n",
        "\n",
        "y_train_wafer[y_train_wafer == -1] = 0\n",
        "y_test_wafer[y_test_wafer == -1] = 0\n",
        "\n",
        "X_train_wafer = X_train_wafer.reshape((-1, 152, 1))\n",
        "X_test_wafer = X_test_wafer.reshape((-1, 152, 1))\n",
        "\n",
        "train_dataset_wafer = prepare_dataset(X_train_wafer, y_train_wafer, batch_size=batch_size)\n",
        "test_dataset_wafer = prepare_dataset(X_test_wafer, y_test_wafer, batch_size=batch_size)\n",
        "\n",
        "num_classes = len(np.unique(y_train_wafer))\n",
        "\n",
        "input_shape_3d = (152, 1)\n",
        "mlp_model = create_mlp_model(input_shape_3d, num_classes)\n",
        "fcn_model = create_fcn_model(input_shape_3d, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PYx_ACFkZ4Y",
        "outputId": "1385c805-28ca-4daf-a8e2-b31b5ae12e87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 0.4483 - accuracy: 0.8427\n",
            "Epoch 1: val_loss improved from inf to 0.34238, saving model to best_model.h5\n",
            "32/32 [==============================] - 2s 38ms/step - loss: 0.4388 - accuracy: 0.8510 - val_loss: 0.3424 - val_accuracy: 0.8921\n",
            "Epoch 2/10\n",
            "11/32 [=========>....................] - ETA: 0s - loss: 0.3431 - accuracy: 0.8949"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29/32 [==========================>...] - ETA: 0s - loss: 0.3048 - accuracy: 0.9095\n",
            "Epoch 2: val_loss improved from 0.34238 to 0.26728, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 34ms/step - loss: 0.3040 - accuracy: 0.9070 - val_loss: 0.2673 - val_accuracy: 0.8921\n",
            "Epoch 3/10\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 0.2594 - accuracy: 0.8998\n",
            "Epoch 3: val_loss improved from 0.26728 to 0.23263, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 35ms/step - loss: 0.2534 - accuracy: 0.9030 - val_loss: 0.2326 - val_accuracy: 0.8921\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.2279 - accuracy: 0.9060\n",
            "Epoch 4: val_loss improved from 0.23263 to 0.20691, saving model to best_model.h5\n",
            "32/32 [==============================] - 2s 51ms/step - loss: 0.2279 - accuracy: 0.9060 - val_loss: 0.2069 - val_accuracy: 0.8934\n",
            "Epoch 5/10\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.1994 - accuracy: 0.9123\n",
            "Epoch 5: val_loss improved from 0.20691 to 0.18562, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 34ms/step - loss: 0.1985 - accuracy: 0.9130 - val_loss: 0.1856 - val_accuracy: 0.9153\n",
            "Epoch 6/10\n",
            "30/32 [===========================>..] - ETA: 0s - loss: 0.1867 - accuracy: 0.9115\n",
            "Epoch 6: val_loss improved from 0.18562 to 0.16725, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 30ms/step - loss: 0.1835 - accuracy: 0.9140 - val_loss: 0.1673 - val_accuracy: 0.9296\n",
            "Epoch 7/10\n",
            "30/32 [===========================>..] - ETA: 0s - loss: 0.1672 - accuracy: 0.9229\n",
            "Epoch 7: val_loss improved from 0.16725 to 0.15101, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 30ms/step - loss: 0.1668 - accuracy: 0.9230 - val_loss: 0.1510 - val_accuracy: 0.9375\n",
            "Epoch 8/10\n",
            "30/32 [===========================>..] - ETA: 0s - loss: 0.1518 - accuracy: 0.9385\n",
            "Epoch 8: val_loss improved from 0.15101 to 0.13770, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 30ms/step - loss: 0.1514 - accuracy: 0.9390 - val_loss: 0.1377 - val_accuracy: 0.9427\n",
            "Epoch 9/10\n",
            "30/32 [===========================>..] - ETA: 0s - loss: 0.1405 - accuracy: 0.9385\n",
            "Epoch 9: val_loss improved from 0.13770 to 0.12543, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 34ms/step - loss: 0.1393 - accuracy: 0.9400 - val_loss: 0.1254 - val_accuracy: 0.9463\n",
            "Epoch 10/10\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 0.1225 - accuracy: 0.9547\n",
            "Epoch 10: val_loss improved from 0.12543 to 0.11466, saving model to best_model.h5\n",
            "32/32 [==============================] - 1s 32ms/step - loss: 0.1243 - accuracy: 0.9540 - val_loss: 0.1147 - val_accuracy: 0.9481\n",
            "193/193 [==============================] - 1s 3ms/step - loss: 0.1147 - accuracy: 0.9481\n"
          ]
        }
      ],
      "source": [
        "mlp_history, mlp_test_loss, mlp_accuracy = train_model(mlp_model, train_dataset_wafer, test_dataset_wafer, epochs=10, learning_rate=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olOiXgJSkzLE",
        "outputId": "af5de72c-cd78-429c-8676-eabaece5e652"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.6911 - accuracy: 0.5230\n",
            "Epoch 1: val_loss improved from inf to 0.62590, saving model to best_model.h5\n",
            "32/32 [==============================] - 30s 873ms/step - loss: 0.6911 - accuracy: 0.5230 - val_loss: 0.6259 - val_accuracy: 0.8921\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.6723 - accuracy: 0.5930\n",
            "Epoch 2: val_loss improved from 0.62590 to 0.56539, saving model to best_model.h5\n",
            "32/32 [==============================] - 18s 562ms/step - loss: 0.6723 - accuracy: 0.5930 - val_loss: 0.5654 - val_accuracy: 0.8921\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.6646 - accuracy: 0.6020\n",
            "Epoch 3: val_loss improved from 0.56539 to 0.51761, saving model to best_model.h5\n",
            "32/32 [==============================] - 27s 864ms/step - loss: 0.6646 - accuracy: 0.6020 - val_loss: 0.5176 - val_accuracy: 0.8921\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.6570 - accuracy: 0.6320\n",
            "Epoch 4: val_loss improved from 0.51761 to 0.48267, saving model to best_model.h5\n",
            "32/32 [==============================] - 17s 537ms/step - loss: 0.6570 - accuracy: 0.6320 - val_loss: 0.4827 - val_accuracy: 0.8921\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.6512 - accuracy: 0.6520\n",
            "Epoch 5: val_loss improved from 0.48267 to 0.45614, saving model to best_model.h5\n",
            "32/32 [==============================] - 18s 572ms/step - loss: 0.6512 - accuracy: 0.6520 - val_loss: 0.4561 - val_accuracy: 0.8921\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.6450 - accuracy: 0.6520\n",
            "Epoch 6: val_loss improved from 0.45614 to 0.43521, saving model to best_model.h5\n",
            "32/32 [==============================] - 27s 865ms/step - loss: 0.6450 - accuracy: 0.6520 - val_loss: 0.4352 - val_accuracy: 0.8921\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.6380 - accuracy: 0.7010\n",
            "Epoch 7: val_loss improved from 0.43521 to 0.41856, saving model to best_model.h5\n",
            "32/32 [==============================] - 18s 576ms/step - loss: 0.6380 - accuracy: 0.7010 - val_loss: 0.4186 - val_accuracy: 0.8921\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.6343 - accuracy: 0.6970\n",
            "Epoch 8: val_loss improved from 0.41856 to 0.40513, saving model to best_model.h5\n",
            "32/32 [==============================] - 27s 872ms/step - loss: 0.6343 - accuracy: 0.6970 - val_loss: 0.4051 - val_accuracy: 0.8921\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.6286 - accuracy: 0.7160\n",
            "Epoch 9: val_loss improved from 0.40513 to 0.40081, saving model to best_model.h5\n",
            "32/32 [==============================] - 18s 589ms/step - loss: 0.6286 - accuracy: 0.7160 - val_loss: 0.4008 - val_accuracy: 0.8921\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.6238 - accuracy: 0.7190\n",
            "Epoch 10: val_loss improved from 0.40081 to 0.39572, saving model to best_model.h5\n",
            "32/32 [==============================] - 27s 873ms/step - loss: 0.6238 - accuracy: 0.7190 - val_loss: 0.3957 - val_accuracy: 0.8921\n",
            "193/193 [==============================] - 12s 60ms/step - loss: 0.3957 - accuracy: 0.8921\n"
          ]
        }
      ],
      "source": [
        "fcn_history, fcn_test_loss, fcn_accuracy = train_model(fcn_model, train_dataset_wafer, test_dataset_wafer, epochs=10, learning_rate=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4zH0ZXQmJhw",
        "outputId": "03446075-9651-457e-b357-075696314066"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fully Connected Network Test Accuracy: 0.8921\n",
            "MLP Test Accuracy: 0.9481\n"
          ]
        }
      ],
      "source": [
        "fcn_test_loss, fcn_test_accuracy = fcn_model.evaluate(X_test_wafer, y_test_wafer, verbose=0)\n",
        "print(f\"Fully Connected Network Test Accuracy: {fcn_test_accuracy:.4f}\")\n",
        "\n",
        "mlp_test_loss, mlp_test_accuracy = mlp_model.evaluate(X_test_wafer, y_test_wafer, verbose=0)\n",
        "print(f\"MLP Test Accuracy: {mlp_test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYGxGXfnG1c6"
      },
      "source": [
        "### Task 3: Time series classification using deep learning 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbqsQIicG_nb"
      },
      "source": [
        "Next, you can try to further improve your model by selecting **two** of the following ideas:\n",
        "- Use Bi-Direction LSTM and CNN networks separately, create two to three layers individually, and concatenate them. This means that until the third (or second) layer, you have two different networks handling the same dataset, and after that, you concatenate the output and finish with any FCN layer. Check [this post](https://stackoverflow.com/questions/59168306/how-to-combine-lstm-and-cnn-in-timeseries-classification) to get inspired.\n",
        "- Apply any sktime's transformer (not attention transformer) first to the dataset and run any deep learning model you already developed in Tasks 2 and 3. In this case, you need to choose at least two transformers and apply them together.\n",
        "- Train the model on multiple similar datasets and test it on one specific test set. Check if the model can be improved if it is trained on multiple datasets (at least five datasets). However, for this, you also need to choose the similar datasets based on their classification and motivate your choise in the report (UCR repository has a specific dataset type such as **AUDIO** or **MOTION**). You could try to crop or pad the time series if you would like to match the sizes.\n",
        "\n",
        "Choose one model you want from the models you have developed in Tasks 1 and 2. Select one idea, try implementing it, and check if you can improve the performance. Note that you do not need to prove that the accuracy scores increase but must explain your trials. Report test scores on three datasets you chose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "sPnS8lZHaWZ7"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(X, y, batch_size=32, shuffle_buffer_size=100):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    if shuffle_buffer_size > 0:\n",
        "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sktime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDhh_2wCRcMO",
        "outputId": "e393e232-808f-48e5-94db-0bb1c019555f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sktime in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: numpy<1.27,>=1.21 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sktime) (24.0)\n",
            "Requirement already satisfied: pandas<2.3.0,>=1.1 in /usr/local/lib/python3.10/dist-packages (from sktime) (2.0.3)\n",
            "Requirement already satisfied: scikit-base<0.8.0 in /usr/local/lib/python3.10/dist-packages (from sktime) (0.7.5)\n",
            "Requirement already satisfied: scikit-learn<1.5.0,>=0.24 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.2.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.11.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.5.0,>=0.24->sktime) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.5.0,>=0.24->sktime) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0,>=1.1->sktime) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sktime.classification.deep_learning.cnn import CNNClassifier\n",
        "from sktime.datasets import load_unit_test\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, Conv1D, GRU, Flatten, Input,LSTM\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Conv1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, Add\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "s7P2giZVAmZ4"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "4NR_NUJCCgeN"
      },
      "outputs": [],
      "source": [
        "from sktime.datasets import load_UCR_UEA_dataset\n",
        "X_train_ecg200, y_train_ecg200 = load_UCR_UEA_dataset(name=\"ECG200\", split=\"train\", return_X_y=True, return_type=\"numpy3D\")\n",
        "X_test_ecg200, y_test_ecg200 = load_UCR_UEA_dataset(name=\"ECG200\", split=\"test\", return_X_y=True, return_type=\"numpy3D\")\n",
        "\n",
        "if not isinstance(y_train_ecg200, np.ndarray):\n",
        "    y_train_ecg200 = np.array(y_train_ecg200)\n",
        "if not isinstance(y_test_ecg200, np.ndarray):\n",
        "    y_test_ecg200 = np.array(y_test_ecg200)\n",
        "\n",
        "y_train_ecg200 = y_train_ecg200.astype('int64')\n",
        "y_test_ecg200 = y_test_ecg200.astype('int64')\n",
        "\n",
        "mean = X_train_ecg200.mean()\n",
        "std = X_train_ecg200.std()\n",
        "\n",
        "X_train_ecg200 = (X_train_ecg200 - mean) / std\n",
        "X_test_ecg200 = (X_test_ecg200 - mean) / std\n",
        "\n",
        "y_train_ecg200[y_train_ecg200 == -1] = 0\n",
        "y_test_ecg200[y_test_ecg200 == -1] = 0\n",
        "\n",
        "X_train_ecg200 = X_train_ecg200.reshape((-1, 96, 1))\n",
        "X_test_ecg200 = X_test_ecg200.reshape((-1, 96, 1))\n",
        "\n",
        "batch_size=32\n",
        "train_dataset_ecg200 = prepare_dataset(X_train_ecg200, y_train_ecg200, batch_size=batch_size)\n",
        "test_dataset_ecg200 = prepare_dataset(X_test_ecg200, y_test_ecg200, batch_size=batch_size)\n",
        "\n",
        "num_classes = len(np.unique(y_train_ecg200))\n",
        "learning_rate=1e-5\n",
        "input_shape_3d = (96, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ARKOIQPX38IQ"
      },
      "outputs": [],
      "source": [
        "def create_model(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    side1 = Bidirectional(LSTM(100, return_sequences=True))(inputs)\n",
        "    side2 = Conv1D(200, kernel_size=3, activation='tanh', padding='same')(inputs)\n",
        "\n",
        "    merged = Add()([side1, side2])\n",
        "    outputs = Conv1D(200, kernel_size=3, activation='relu', padding='same')(merged)\n",
        "    outputs = GlobalMaxPooling1D()(outputs)\n",
        "    outputs = Dense(100, activation='relu')(outputs)\n",
        "    outputs = Dense(1, activation='sigmoid')(outputs)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "U_XAomRl4IXj"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_dataset, test_dataset, epochs, learning_rate):\n",
        "    optimizer = Adam(learning_rate=learning_rate, clipvalue=0.5)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=3, verbose=1),\n",
        "        ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        epochs=epochs,\n",
        "        validation_data=test_dataset,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        model.load_weights('best_model.h5')\n",
        "    except IOError as e:\n",
        "        print(f\"Checkpoint file not found: {e}. Using model as is after training.\")\n",
        "\n",
        "    test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "    return history, test_loss, test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QK8wU5C4J1V",
        "outputId": "2d980635-134a-4bca-ebcd-5ec382f65d57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.7421 - accuracy: 0.3500\n",
            "Epoch 1: val_loss improved from inf to 0.66947, saving model to best_model.h5\n",
            "4/4 [==============================] - 13s 2s/step - loss: 0.7421 - accuracy: 0.3500 - val_loss: 0.6695 - val_accuracy: 0.6400\n",
            "Epoch 2/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - ETA: 0s - loss: 0.6433 - accuracy: 0.6900\n",
            "Epoch 2: val_loss improved from 0.66947 to 0.64436, saving model to best_model.h5\n",
            "4/4 [==============================] - 2s 591ms/step - loss: 0.6433 - accuracy: 0.6900 - val_loss: 0.6444 - val_accuracy: 0.6400\n",
            "Epoch 3/15\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.6055 - accuracy: 0.6900\n",
            "Epoch 3: val_loss improved from 0.64436 to 0.63907, saving model to best_model.h5\n",
            "4/4 [==============================] - 2s 586ms/step - loss: 0.6055 - accuracy: 0.6900 - val_loss: 0.6391 - val_accuracy: 0.6400\n",
            "Epoch 4/15\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.5925 - accuracy: 0.6900\n",
            "Epoch 4: val_loss did not improve from 0.63907\n",
            "4/4 [==============================] - 2s 574ms/step - loss: 0.5925 - accuracy: 0.6900 - val_loss: 0.6452 - val_accuracy: 0.6400\n",
            "Epoch 5/15\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.5862 - accuracy: 0.6900\n",
            "Epoch 5: val_loss improved from 0.63907 to 0.63613, saving model to best_model.h5\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.5862 - accuracy: 0.6900 - val_loss: 0.6361 - val_accuracy: 0.6400\n",
            "Epoch 6/15\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.5786 - accuracy: 0.6900\n",
            "Epoch 6: val_loss improved from 0.63613 to 0.62904, saving model to best_model.h5\n",
            "4/4 [==============================] - 2s 584ms/step - loss: 0.5786 - accuracy: 0.6900 - val_loss: 0.6290 - val_accuracy: 0.6400\n",
            "Epoch 7/15\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.5732 - accuracy: 0.6900\n",
            "Epoch 7: val_loss improved from 0.62904 to 0.62322, saving model to best_model.h5\n",
            "4/4 [==============================] - 3s 666ms/step - loss: 0.5732 - accuracy: 0.6900 - val_loss: 0.6232 - val_accuracy: 0.6400\n",
            "Epoch 8/15\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.5683 - accuracy: 0.6900\n",
            "Epoch 8: val_loss improved from 0.62322 to 0.62003, saving model to best_model.h5\n",
            "4/4 [==============================] - 2s 666ms/step - loss: 0.5683 - accuracy: 0.6900 - val_loss: 0.6200 - val_accuracy: 0.6400\n",
            "Epoch 9/15\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.5637 - accuracy: 0.6900\n",
            "Epoch 9: val_loss improved from 0.62003 to 0.61331, saving model to best_model.h5\n",
            "4/4 [==============================] - 2s 668ms/step - loss: 0.5637 - accuracy: 0.6900 - val_loss: 0.6133 - val_accuracy: 0.6400\n",
            "Epoch 10/15\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.5603 - accuracy: 0.6900\n",
            "Epoch 10: val_loss improved from 0.61331 to 0.60433, saving model to best_model.h5\n",
            "4/4 [==============================] - 3s 865ms/step - loss: 0.5603 - accuracy: 0.6900 - val_loss: 0.6043 - val_accuracy: 0.6400\n",
            "Epoch 11/15\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.5550 - accuracy: 0.6900\n",
            "Epoch 11: val_loss improved from 0.60433 to 0.59834, saving model to best_model.h5\n",
            "4/4 [==============================] - 2s 473ms/step - loss: 0.5550 - accuracy: 0.6900 - val_loss: 0.5983 - val_accuracy: 0.6400\n",
            "Epoch 12/15\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.5572 - accuracy: 0.6900\n",
            "Epoch 12: val_loss improved from 0.59834 to 0.59343, saving model to best_model.h5\n",
            "4/4 [==============================] - 3s 698ms/step - loss: 0.5572 - accuracy: 0.6900 - val_loss: 0.5934 - val_accuracy: 0.6400\n",
            "Epoch 13/15\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.5506 - accuracy: 0.6900\n",
            "Epoch 13: val_loss improved from 0.59343 to 0.59294, saving model to best_model.h5\n",
            "4/4 [==============================] - 2s 577ms/step - loss: 0.5506 - accuracy: 0.6900 - val_loss: 0.5929 - val_accuracy: 0.6400\n",
            "Epoch 14/15\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.5419 - accuracy: 0.6900\n",
            "Epoch 14: val_loss improved from 0.59294 to 0.59246, saving model to best_model.h5\n",
            "4/4 [==============================] - 3s 820ms/step - loss: 0.5419 - accuracy: 0.6900 - val_loss: 0.5925 - val_accuracy: 0.6400\n",
            "Epoch 15/15\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.5390 - accuracy: 0.6900\n",
            "Epoch 15: val_loss did not improve from 0.59246\n",
            "4/4 [==============================] - 3s 608ms/step - loss: 0.5390 - accuracy: 0.6900 - val_loss: 0.5926 - val_accuracy: 0.6400\n",
            "4/4 [==============================] - 1s 170ms/step - loss: 0.5925 - accuracy: 0.6400\n",
            "Test accuracy: 0.6399999856948853\n"
          ]
        }
      ],
      "source": [
        "input_shape = (96, 1)\n",
        "\n",
        "model = create_model(input_shape)\n",
        "history, test_loss, test_accuracy = train_model(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset_ecg200,\n",
        "    test_dataset=test_dataset_ecg200,\n",
        "    epochs=15,\n",
        "    learning_rate=1e-4\n",
        ")\n",
        "\n",
        "print(f\"Test accuracy: {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "BZjoZkS7BWcj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b7c56d8-74e1-4ce4-cae2-4bbada7b4a38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy on ECG dataset: 0.6400\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test Accuracy on ECG dataset: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgAIe156w-Wr",
        "outputId": "635203ea-c44c-4874-a5b7-4f086b7f26a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 1, 286)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "X_train_coffee, y_train_coffee = load_UCR_UEA_dataset(name=\"Coffee\", split=\"train\", return_X_y=True, return_type=\"numpy3D\")\n",
        "X_test_coffee, y_test_coffee = load_UCR_UEA_dataset(name=\"Coffee\", split=\"test\", return_X_y=True, return_type=\"numpy3D\")\n",
        "X_train_coffee.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "GYLeHXIV4kf_"
      },
      "outputs": [],
      "source": [
        "#coffee dataset\n",
        "X_train_coffee, y_train_coffee = load_UCR_UEA_dataset(name=\"Coffee\", split=\"train\", return_X_y=True, return_type=\"numpy3D\")\n",
        "X_test_coffee, y_test_coffee = load_UCR_UEA_dataset(name=\"Coffee\", split=\"test\", return_X_y=True, return_type=\"numpy3D\")\n",
        "\n",
        "if not isinstance(y_train_coffee, np.ndarray):\n",
        "    y_train_coffee = np.array(y_train_coffee)\n",
        "if not isinstance(y_test_coffee, np.ndarray):\n",
        "    y_test_coffee = np.array(y_test_coffee)\n",
        "\n",
        "y_train_coffee = y_train_coffee.astype('int64')\n",
        "y_test_coffee = y_test_coffee.astype('int64')\n",
        "\n",
        "mean = X_train_coffee.mean()\n",
        "std = X_train_coffee.std()\n",
        "\n",
        "X_train_coffee = (X_train_coffee - mean) / std\n",
        "X_test_coffee = (X_test_coffee - mean) / std\n",
        "\n",
        "y_train_coffee[y_train_coffee == -1] = 0\n",
        "y_test_coffee[y_test_coffee == -1] = 0\n",
        "\n",
        "X_train_coffee = X_train_coffee.reshape((-1, 286, 1))\n",
        "X_test_coffee = X_test_coffee.reshape((-1, 286, 1))\n",
        "\n",
        "train_dataset_coffee = prepare_dataset(X_train_coffee, y_train_coffee, batch_size=batch_size)\n",
        "test_dataset_coffee = prepare_dataset(X_test_coffee, y_test_coffee, batch_size=batch_size)\n",
        "\n",
        "input_shape_3d = (286, 1)\n",
        "num_classes = len(np.unique(y_train_coffee))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39ZozulG5JG9",
        "outputId": "4c685d72-075e-4050-8537-fdf4525ef0c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6977 - accuracy: 0.3571\n",
            "Epoch 1: val_loss improved from inf to 0.69474, saving model to best_model.h5\n",
            "1/1 [==============================] - 16s 16s/step - loss: 0.6977 - accuracy: 0.3571 - val_loss: 0.6947 - val_accuracy: 0.3929\n",
            "Epoch 2/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r1/1 [==============================] - ETA: 0s - loss: 0.6944 - accuracy: 0.3929\n",
            "Epoch 2: val_loss improved from 0.69474 to 0.69241, saving model to best_model.h5\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6944 - accuracy: 0.3929 - val_loss: 0.6924 - val_accuracy: 0.5714\n",
            "Epoch 3/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6917 - accuracy: 0.6071\n",
            "Epoch 3: val_loss improved from 0.69241 to 0.68979, saving model to best_model.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.6917 - accuracy: 0.6071 - val_loss: 0.6898 - val_accuracy: 0.9286\n",
            "Epoch 4/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6892 - accuracy: 0.8929\n",
            "Epoch 4: val_loss improved from 0.68979 to 0.68771, saving model to best_model.h5\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6892 - accuracy: 0.8929 - val_loss: 0.6877 - val_accuracy: 0.6071\n",
            "Epoch 5/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6873 - accuracy: 0.6071\n",
            "Epoch 5: val_loss improved from 0.68771 to 0.68582, saving model to best_model.h5\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6873 - accuracy: 0.6071 - val_loss: 0.6858 - val_accuracy: 0.5357\n",
            "Epoch 6/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6855 - accuracy: 0.5357\n",
            "Epoch 6: val_loss improved from 0.68582 to 0.68431, saving model to best_model.h5\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6855 - accuracy: 0.5357 - val_loss: 0.6843 - val_accuracy: 0.6786\n",
            "Epoch 7/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6839 - accuracy: 0.6429\n",
            "Epoch 7: val_loss improved from 0.68431 to 0.68301, saving model to best_model.h5\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6839 - accuracy: 0.6429 - val_loss: 0.6830 - val_accuracy: 0.8571\n",
            "Epoch 8/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6823 - accuracy: 0.8571\n",
            "Epoch 8: val_loss improved from 0.68301 to 0.68173, saving model to best_model.h5\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6823 - accuracy: 0.8571 - val_loss: 0.6817 - val_accuracy: 0.9643\n",
            "Epoch 9/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6807 - accuracy: 0.9643\n",
            "Epoch 9: val_loss improved from 0.68173 to 0.68045, saving model to best_model.h5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.6807 - accuracy: 0.9643 - val_loss: 0.6804 - val_accuracy: 1.0000\n",
            "Epoch 10/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6792 - accuracy: 1.0000\n",
            "Epoch 10: val_loss improved from 0.68045 to 0.67911, saving model to best_model.h5\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6792 - accuracy: 1.0000 - val_loss: 0.6791 - val_accuracy: 1.0000\n",
            "Epoch 11/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6777 - accuracy: 1.0000\n",
            "Epoch 11: val_loss improved from 0.67911 to 0.67764, saving model to best_model.h5\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6777 - accuracy: 1.0000 - val_loss: 0.6776 - val_accuracy: 1.0000\n",
            "Epoch 12/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6763 - accuracy: 1.0000\n",
            "Epoch 12: val_loss improved from 0.67764 to 0.67599, saving model to best_model.h5\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6763 - accuracy: 1.0000 - val_loss: 0.6760 - val_accuracy: 1.0000\n",
            "Epoch 13/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6747 - accuracy: 0.9643\n",
            "Epoch 13: val_loss improved from 0.67599 to 0.67441, saving model to best_model.h5\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6747 - accuracy: 0.9643 - val_loss: 0.6744 - val_accuracy: 0.9643\n",
            "Epoch 14/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6733 - accuracy: 0.9643\n",
            "Epoch 14: val_loss improved from 0.67441 to 0.67271, saving model to best_model.h5\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6733 - accuracy: 0.9643 - val_loss: 0.6727 - val_accuracy: 0.9643\n",
            "Epoch 15/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6717 - accuracy: 0.9643\n",
            "Epoch 15: val_loss improved from 0.67271 to 0.67140, saving model to best_model.h5\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6717 - accuracy: 0.9643 - val_loss: 0.6714 - val_accuracy: 0.9643\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 0.6714 - accuracy: 0.9643\n",
            "Test accuracy: 0.9642857313156128\n"
          ]
        }
      ],
      "source": [
        "model = create_model(input_shape_3d)\n",
        "history, test_loss, test_accuracy = train_model(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset_coffee,\n",
        "    test_dataset=test_dataset_coffee,\n",
        "    epochs=15,\n",
        "    learning_rate=1e-4\n",
        ")\n",
        "\n",
        "print(f\"Test accuracy: {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "dcqYkLxmBkMc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96b2b644-c1f4-4ac3-b8ab-1f7a50042901"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy on coffee dataset: 0.9643\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test Accuracy on coffee dataset: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "s0B_tp0N6CrK"
      },
      "outputs": [],
      "source": [
        "X_train_wafer, y_train_wafer = load_UCR_UEA_dataset(name=\"Wafer\", split=\"train\", return_X_y=True, return_type=\"numpy3D\")\n",
        "X_test_wafer, y_test_wafer = load_UCR_UEA_dataset(name=\"Wafer\", split=\"test\", return_X_y=True, return_type=\"numpy3D\")\n",
        "\n",
        "if not isinstance(y_train_wafer, np.ndarray):\n",
        "    y_train_wafer = np.array(y_train_wafer)\n",
        "if not isinstance(y_test_wafer, np.ndarray):\n",
        "    y_test_wafer = np.array(y_test_wafer)\n",
        "\n",
        "y_train_wafer = y_train_wafer.astype('int64')\n",
        "y_test_wafer = y_test_wafer.astype('int64')\n",
        "\n",
        "mean = X_train_wafer.mean()\n",
        "std = X_train_wafer.std()\n",
        "\n",
        "X_train_wafer = (X_train_wafer - mean) / std\n",
        "X_test_wafer = (X_test_wafer - mean) / std\n",
        "\n",
        "y_train_wafer[y_train_wafer == -1] = 0\n",
        "y_test_wafer[y_test_wafer == -1] = 0\n",
        "\n",
        "X_train_wafer = X_train_wafer.reshape((-1, 152, 1))\n",
        "X_test_wafer = X_test_wafer.reshape((-1, 152, 1))\n",
        "\n",
        "train_dataset_wafer = prepare_dataset(X_train_wafer, y_train_wafer, batch_size=batch_size)\n",
        "test_dataset_wafer = prepare_dataset(X_test_wafer, y_test_wafer, batch_size=batch_size)\n",
        "\n",
        "num_classes = len(np.unique(y_train_wafer))\n",
        "\n",
        "input_shape_3d = (152, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC0mbpKW6GwM",
        "outputId": "2980aa45-a118-45dd-95f9-2321cc127c07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.3869 - accuracy: 0.9030\n",
            "Epoch 1: val_loss improved from inf to 0.34338, saving model to best_model.h5\n",
            "32/32 [==============================] - 52s 1s/step - loss: 0.3869 - accuracy: 0.9030 - val_loss: 0.3434 - val_accuracy: 0.8921\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - ETA: 0s - loss: 0.3205 - accuracy: 0.9030\n",
            "Epoch 2: val_loss improved from 0.34338 to 0.33073, saving model to best_model.h5\n",
            "32/32 [==============================] - 38s 1s/step - loss: 0.3205 - accuracy: 0.9030 - val_loss: 0.3307 - val_accuracy: 0.8921\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.3078 - accuracy: 0.9030\n",
            "Epoch 3: val_loss improved from 0.33073 to 0.32118, saving model to best_model.h5\n",
            "32/32 [==============================] - 33s 1s/step - loss: 0.3078 - accuracy: 0.9030 - val_loss: 0.3212 - val_accuracy: 0.8921\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.2983 - accuracy: 0.9030\n",
            "Epoch 4: val_loss improved from 0.32118 to 0.31198, saving model to best_model.h5\n",
            "32/32 [==============================] - 30s 946ms/step - loss: 0.2983 - accuracy: 0.9030 - val_loss: 0.3120 - val_accuracy: 0.8921\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.2878 - accuracy: 0.9030\n",
            "Epoch 5: val_loss improved from 0.31198 to 0.30515, saving model to best_model.h5\n",
            "32/32 [==============================] - 29s 932ms/step - loss: 0.2878 - accuracy: 0.9030 - val_loss: 0.3052 - val_accuracy: 0.8921\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.2769 - accuracy: 0.9030\n",
            "Epoch 6: val_loss improved from 0.30515 to 0.29117, saving model to best_model.h5\n",
            "32/32 [==============================] - 34s 1s/step - loss: 0.2769 - accuracy: 0.9030 - val_loss: 0.2912 - val_accuracy: 0.8921\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.2596 - accuracy: 0.9030\n",
            "Epoch 7: val_loss improved from 0.29117 to 0.27223, saving model to best_model.h5\n",
            "32/32 [==============================] - 34s 1s/step - loss: 0.2596 - accuracy: 0.9030 - val_loss: 0.2722 - val_accuracy: 0.8921\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.2229 - accuracy: 0.9030\n",
            "Epoch 8: val_loss improved from 0.27223 to 0.20692, saving model to best_model.h5\n",
            "32/32 [==============================] - 28s 896ms/step - loss: 0.2229 - accuracy: 0.9030 - val_loss: 0.2069 - val_accuracy: 0.8933\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.1411 - accuracy: 0.9500\n",
            "Epoch 9: val_loss improved from 0.20692 to 0.11345, saving model to best_model.h5\n",
            "32/32 [==============================] - 35s 1s/step - loss: 0.1411 - accuracy: 0.9500 - val_loss: 0.1134 - val_accuracy: 0.9562\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.9750\n",
            "Epoch 10: val_loss improved from 0.11345 to 0.07944, saving model to best_model.h5\n",
            "32/32 [==============================] - 29s 937ms/step - loss: 0.0855 - accuracy: 0.9750 - val_loss: 0.0794 - val_accuracy: 0.9711\n",
            "193/193 [==============================] - 19s 96ms/step - loss: 0.0794 - accuracy: 0.9711\n",
            "Test accuracy: 0.9711226224899292\n"
          ]
        }
      ],
      "source": [
        "model = create_model(input_shape_3d)\n",
        "\n",
        "history, test_loss, test_accuracy = train_model(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset_wafer,\n",
        "    test_dataset=test_dataset_wafer,\n",
        "    epochs=10,\n",
        "    learning_rate=1e-4\n",
        ")\n",
        "\n",
        "print(f\"Test accuracy: {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "chk1GsnQBtBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bab3cd6-1d82-44be-e5e9-360132722aa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy on wafer dataset: 0.9711\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test Accuracy on wafer dataset: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJgMhMH1MFVR",
        "outputId": "c578daa8-f9ae-4d44-a91f-ce23a81cd7e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sktime\n",
            "  Downloading sktime-0.27.0-py3-none-any.whl (21.9 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m21.9/21.9 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.27,>=1.21 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sktime) (24.0)\n",
            "Requirement already satisfied: pandas<2.2.0,>=1.1 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.5.3)\n",
            "Collecting scikit-base<0.8.0 (from sktime)\n",
            "  Downloading scikit_base-0.7.5-py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m128.8/128.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<1.5.0,>=0.24 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.2.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.11.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.0,>=1.1->sktime) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.0,>=1.1->sktime) (2023.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.5.0,>=0.24->sktime) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.5.0,>=0.24->sktime) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<2.2.0,>=1.1->sktime) (1.16.0)\n",
            "Installing collected packages: scikit-base, sktime\n",
            "Successfully installed scikit-base-0.7.5 sktime-0.27.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sktime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cekb8mk2CdR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-MgtI5JieED"
      },
      "outputs": [],
      "source": [
        "#Transformation using BoxCoxTransformer and Rocket\n",
        "from sktime.datasets import load_UCR_UEA_dataset\n",
        "from sktime.transformations.series.boxcox import BoxCoxTransformer\n",
        "from sktime.transformations.panel.rocket import Rocket\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "X_train, y_train = load_UCR_UEA_dataset(name=\"ECG200\", split=\"train\", return_X_y=True, return_type=\"numpy3D\")\n",
        "X_test, y_test = load_UCR_UEA_dataset(name=\"ECG200\", split=\"test\", return_X_y=True, return_type=\"numpy3D\")\n",
        "\n",
        "y_train = y_train.astype('int64')\n",
        "y_test = y_test.astype('int64')\n",
        "\n",
        "y_train[y_train == -1] = 0\n",
        "y_test[y_test == -1] = 0\n",
        "\n",
        "boxcox_trans = BoxCoxTransformer()\n",
        "rocket_trans = Rocket()\n",
        "\n",
        "X_train_boxcox = boxcox_trans.fit_transform(X_train)\n",
        "X_test_boxcox = boxcox_trans.transform(X_test)\n",
        "\n",
        "rocket_trans.fit(X_train_boxcox)\n",
        "X_train_rocket = rocket_trans.transform(X_train_boxcox)\n",
        "X_test_rocket = rocket_trans.transform(X_test_boxcox)\n",
        "\n",
        "num_classes = len(np.unique(y_train))\n",
        "num_features = X_train_rocket.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDQIQIDUs_F-"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqcAvvDftG6E"
      },
      "outputs": [],
      "source": [
        "y_train[y_train == -1] = 0\n",
        "y_test[y_test == -1] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XvALMFxjjhp"
      },
      "outputs": [],
      "source": [
        "def create_mlp_model(input_shape, num_classes):\n",
        "    output_activation = 'sigmoid' if num_classes == 2 else 'softmax'\n",
        "\n",
        "    output_neurons = 1 if num_classes == 2 else num_classes\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=input_shape),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.1),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(output_neurons, activation=output_activation)\n",
        "    ])\n",
        "\n",
        "    chosen_loss = 'binary_crossentropy' if num_classes == 2 else 'categorical_crossentropy'\n",
        "\n",
        "    optimizer = Adam(learning_rate=1e-4)\n",
        "    model.compile(optimizer=optimizer, loss=chosen_loss, metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRbkNYQBuGAs",
        "outputId": "d8efa994-80f7-4ed4-b122-a49fd6b0df6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train_rocket shape: (100, 20000)\n",
            "y_train shape: (100,)\n",
            "X_test_rocket shape: (100, 20000)\n",
            "y_test shape: (100,)\n"
          ]
        }
      ],
      "source": [
        "print(\"X_train_rocket shape:\", X_train_rocket.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_test_rocket shape:\", X_test_rocket.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AbI1qKsjtfX",
        "outputId": "09baf30c-db50-4034-9ca7-6c339095db60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 2s 175ms/step - loss: 6.4375 - accuracy: 0.6300 - val_loss: 0.7370 - val_accuracy: 0.5300\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 79ms/step - loss: 3.0892 - accuracy: 0.5000 - val_loss: 4.4113 - val_accuracy: 0.6400\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 83ms/step - loss: 3.7889 - accuracy: 0.6800 - val_loss: 1.3375 - val_accuracy: 0.6400\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 80ms/step - loss: 2.1252 - accuracy: 0.5700 - val_loss: 0.6028 - val_accuracy: 0.7300\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 77ms/step - loss: 1.9776 - accuracy: 0.6300 - val_loss: 0.6128 - val_accuracy: 0.7300\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 87ms/step - loss: 1.6946 - accuracy: 0.4800 - val_loss: 0.7803 - val_accuracy: 0.7200\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 88ms/step - loss: 1.4273 - accuracy: 0.6700 - val_loss: 0.8360 - val_accuracy: 0.7000\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 91ms/step - loss: 1.0481 - accuracy: 0.7200 - val_loss: 0.7075 - val_accuracy: 0.7300\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 90ms/step - loss: 1.1737 - accuracy: 0.6300 - val_loss: 0.7292 - val_accuracy: 0.7200\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 80ms/step - loss: 0.9171 - accuracy: 0.6900 - val_loss: 1.1429 - val_accuracy: 0.6500\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "input_shape = X_train_rocket.shape[1:]\n",
        "\n",
        "mlp_model = create_mlp_model(input_shape, num_classes)\n",
        "\n",
        "history = mlp_model.fit(\n",
        "X_train_rocket, y_train,\n",
        "validation_data=(X_test_rocket, y_test),\n",
        "epochs=10,\n",
        "batch_size=32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ck-UhktL83C",
        "outputId": "4fc8ca7f-94a5-4ca2-e6e8-8bb014fc58b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 1.1429\n",
            "Test Accuracy: 0.6500\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_accuracy = mlp_model.evaluate(X_test_rocket, y_test, verbose=0)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhRVlx6IMplW"
      },
      "outputs": [],
      "source": [
        "#Same on coffee dataset\n",
        "from sktime.datasets import load_UCR_UEA_dataset\n",
        "from sktime.transformations.series.boxcox import BoxCoxTransformer\n",
        "from sktime.transformations.panel.rocket import Rocket\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "X_train, y_train = load_UCR_UEA_dataset(name=\"Coffee\", split=\"train\", return_X_y=True, return_type=\"numpy3D\")\n",
        "X_test, y_test = load_UCR_UEA_dataset(name=\"Coffee\", split=\"test\", return_X_y=True, return_type=\"numpy3D\")\n",
        "\n",
        "y_train = y_train.astype('int64')\n",
        "y_test = y_test.astype('int64')\n",
        "\n",
        "y_train[y_train == -1] = 0\n",
        "y_test[y_test == -1] = 0\n",
        "\n",
        "boxcox_trans = BoxCoxTransformer()\n",
        "rocket_trans = Rocket()\n",
        "\n",
        "X_train_boxcox = boxcox_trans.fit_transform(X_train)\n",
        "X_test_boxcox = boxcox_trans.transform(X_test)\n",
        "\n",
        "rocket_trans.fit(X_train_boxcox)\n",
        "X_train_rocket = rocket_trans.transform(X_train_boxcox)\n",
        "X_test_rocket = rocket_trans.transform(X_test_boxcox)\n",
        "\n",
        "num_classes = len(np.unique(y_train))\n",
        "num_features = X_train_rocket.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zR7zSpOM_9G"
      },
      "outputs": [],
      "source": [
        "y_train[y_train == -1] = 0\n",
        "y_test[y_test == -1] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kw3OKigO6pX"
      },
      "outputs": [],
      "source": [
        "def create_mlp_model(input_shape, num_classes):\n",
        "    output_activation = 'sigmoid' if num_classes == 2 else 'softmax'\n",
        "\n",
        "    output_neurons = 1 if num_classes == 2 else num_classes\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=input_shape),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.1),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(output_neurons, activation=output_activation)\n",
        "    ])\n",
        "\n",
        "    chosen_loss = 'binary_crossentropy' if num_classes == 2 else 'categorical_crossentropy'\n",
        "\n",
        "    optimizer = Adam(learning_rate=1e-4)\n",
        "    model.compile(optimizer=optimizer, loss=chosen_loss, metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHu_xvRfPDdd",
        "outputId": "73433741-2ed7-43e7-eac0-f211c0033029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 1s 1s/step - loss: 5.2224 - accuracy: 0.5000 - val_loss: 18.5939 - val_accuracy: 0.4643\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 17.6225 - accuracy: 0.5000 - val_loss: 14.5969 - val_accuracy: 0.4643\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 13.3944 - accuracy: 0.5000 - val_loss: 7.6235 - val_accuracy: 0.4643\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 5.8456 - accuracy: 0.5000 - val_loss: 0.7723 - val_accuracy: 0.4643\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 2.4441 - accuracy: 0.5714 - val_loss: 3.1969 - val_accuracy: 0.5357\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 5.3080 - accuracy: 0.3929 - val_loss: 3.3062 - val_accuracy: 0.5357\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 5.5706 - accuracy: 0.4643 - val_loss: 1.6731 - val_accuracy: 0.5357\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 3.7284 - accuracy: 0.4643 - val_loss: 0.6787 - val_accuracy: 0.5714\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 4.9299 - accuracy: 0.3214 - val_loss: 2.0783 - val_accuracy: 0.4643\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 2.5758 - accuracy: 0.4286 - val_loss: 3.1160 - val_accuracy: 0.4643\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "input_shape = X_train_rocket.shape[1:]\n",
        "mlp_model = create_mlp_model(input_shape, num_classes)\n",
        "\n",
        "history = mlp_model.fit(\n",
        "X_train_rocket, y_train,\n",
        "validation_data=(X_test_rocket, y_test),\n",
        "epochs=10,\n",
        "batch_size=32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnjB-s6rPTYb",
        "outputId": "ce654b54-1ae6-42bc-fdcf-0da13fd05ce3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 3.1160\n",
            "Test Accuracy: 0.4643\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_accuracy = mlp_model.evaluate(X_test_rocket, y_test, verbose=0)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sktime.datasets import load_UCR_UEA_dataset\n",
        "from sktime.transformations.series.boxcox import BoxCoxTransformer\n",
        "from sktime.transformations.panel.rocket import Rocket\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "X_train, y_train = load_UCR_UEA_dataset(name=\"BirdChicken\", split=\"train\", return_X_y=True, return_type=\"numpy3D\")\n",
        "X_test, y_test = load_UCR_UEA_dataset(name=\"BirdChicken\", split=\"test\", return_X_y=True, return_type=\"numpy3D\")\n",
        "y_train = y_train.astype('int64')\n",
        "y_test = y_test.astype('int64')\n",
        "\n",
        "y_train[y_train == 2] = 0\n",
        "y_test[y_test == 2] = 0\n",
        "\n",
        "boxcox_trans = BoxCoxTransformer()\n",
        "rocket_trans = Rocket()\n",
        "\n",
        "X_train_boxcox = boxcox_trans.fit_transform(X_train)\n",
        "X_test_boxcox = boxcox_trans.transform(X_test)\n",
        "\n",
        "rocket_trans.fit(X_train_boxcox)\n",
        "X_train_rocket = rocket_trans.transform(X_train_boxcox)\n",
        "X_test_rocket = rocket_trans.transform(X_test_boxcox)\n",
        "\n",
        "num_classes = len(np.unique(y_train))\n",
        "num_features = X_train_rocket.shape[1]"
      ],
      "metadata": {
        "id": "A129QqarTvK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mlp_model(input_shape, num_classes):\n",
        "    output_activation = 'sigmoid' if num_classes == 2 else 'softmax'\n",
        "\n",
        "    output_neurons = 1 if num_classes == 2 else num_classes\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=input_shape),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.1),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(output_neurons, activation=output_activation)\n",
        "    ])\n",
        "\n",
        "    chosen_loss = 'binary_crossentropy' if num_classes == 2 else 'categorical_crossentropy'\n",
        "\n",
        "    optimizer = Adam(learning_rate=1e-4)\n",
        "    model.compile(optimizer=optimizer, loss=chosen_loss, metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "R3kQcPDTU43n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "input_shape = X_train_rocket.shape[1:]\n",
        "mlp_model = create_mlp_model(input_shape, num_classes)\n",
        "\n",
        "history = mlp_model.fit(\n",
        "X_train_rocket, y_train,\n",
        "validation_data=(X_test_rocket, y_test),\n",
        "epochs=10,\n",
        "batch_size=32\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWMi-3C5VBAh",
        "outputId": "2e0e3923-b3a0-40f6-d266-fad9c47e2443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.5566 - accuracy: 0.4500 - val_loss: 12.1570 - val_accuracy: 0.5000\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 12.2112 - accuracy: 0.5000 - val_loss: 5.8757 - val_accuracy: 0.5000\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 4.4869 - accuracy: 0.5500 - val_loss: 0.8871 - val_accuracy: 0.5000\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 1.3882 - accuracy: 0.6000 - val_loss: 3.4902 - val_accuracy: 0.5000\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 4.0473 - accuracy: 0.3500 - val_loss: 2.9224 - val_accuracy: 0.5000\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 3.8088 - accuracy: 0.4500 - val_loss: 1.0655 - val_accuracy: 0.5000\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 2.2386 - accuracy: 0.5500 - val_loss: 0.7328 - val_accuracy: 0.4500\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 4.8321 - accuracy: 0.3000 - val_loss: 0.9186 - val_accuracy: 0.5000\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 2.3870 - accuracy: 0.6000 - val_loss: 1.6755 - val_accuracy: 0.5000\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 3.0619 - accuracy: 0.5000 - val_loss: 1.8238 - val_accuracy: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = mlp_model.evaluate(X_test_rocket, y_test, verbose=0)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4M6a7nMVHor",
        "outputId": "bb9f362e-044d-416f-ea24-15c0203b370c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.8238\n",
            "Test Accuracy: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYOXtHBTFZ_E"
      },
      "source": [
        "### Task 4: Time series classification using sktime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkEhcRaAGwPW"
      },
      "source": [
        "We can use RandomizedSearch to find optimal parameter options on pipelines. However, sktime's pipeline does not support scikit-learn's classifiers such as DecisionTree or RandomForest well. However, sometimes we would like to use the output of the sktime transformer (e.g., catch22) to train scikit-learn models such as RandomForest. Sktime supports this with `SklearnClassifierPipeline` to put a  scikit-learn classifier and sktime's transformer together and you need to implement it.\n",
        "\n",
        "Pick one classifier from scikit-learn (that can be anything! e.g., Decision Tree or Logistic Regressor) and two transformers from sktime and create `SklearnClassifierPipeline.` As we tried in this lab, that can be **Rocket with RandomForest** or **Catch22 with DecisionTree**. Pick one parameter from each module (in total, three, one from the classifier and two from two transformers) and run a randomized search on the pipeline you define and report the test score of the best model found by the randomized Search. Compare your best score to the score from the same model with the default setting.\n",
        "\n",
        "Task 4 involves a time-consuming process, so you can only choose **one dataset** to perform the task above. Also, note that you do not need to perform better by conducting a randomized search for this task (but still good to try!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KccHsEFbFcPO"
      },
      "outputs": [],
      "source": [
        "from sktime.transformations.panel.rocket import Rocket\n",
        "from sktime.transformations.panel.catch22 import Catch22\n",
        "from sktime.pipeline import make_pipeline\n",
        "from sktime.classification.compose import SklearnClassifierPipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sktime.datasets import load_UCR_UEA_dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sktime.transformations.series.exponent import ExponentTransformer\n",
        "\n",
        "X_train, y_train = load_UCR_UEA_dataset(name=\"Coffee\", split=\"train\", return_X_y=True)\n",
        "X_test, y_test = load_UCR_UEA_dataset(name=\"Coffee\", split=\"test\", return_X_y=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Tg6x5cXLRoA"
      },
      "outputs": [],
      "source": [
        "from sktime.pipeline import make_pipeline\n",
        "\n",
        "rocket_transformer = Rocket()\n",
        "rf_classifier = RandomForestClassifier()\n",
        "square = ExponentTransformer()\n",
        "\n",
        "pipeline = make_pipeline(rocket_transformer,square , rf_classifier)\n",
        "\n",
        "param_grid = {\n",
        "    'rocketclassifier__num_kernels': [1, 10, 20, 100, 1000],\n",
        "    'exponenttransformer__power': [1,2,3,4,5],\n",
        "    'randomforestclassifier__n_estimators': [100, 200, 300]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPWbgARzLeOz"
      },
      "outputs": [],
      "source": [
        "random_search = RandomizedSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=3,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    random_state=42)\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "test_score = accuracy_score(y_test, y_pred)\n",
        "default_model = SklearnClassifierPipeline(\n",
        "    RandomForestClassifier(),[\n",
        "    ('transformer1', Rocket()),\n",
        "    ('transformer2', ExponentTransformer())]\n",
        ")\n",
        "default_model.fit(X_train, y_train)\n",
        "default_pred = default_model.predict(X_test)\n",
        "default_score = accuracy_score(y_test, default_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuV5EIbIfOUx",
        "outputId": "e45f6a0a-3d9f-410f-a242-5b0124d7243c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test score of the best model: 1.0\n",
            "Test score with default settings: 1.0\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test score of the best model: {test_score}\")\n",
        "print(f\"Test score with default settings: {default_score}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0od-gD76TlmO"
      },
      "source": [
        "### Task 5: Multivariate time series classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe_XHdDJeMVo"
      },
      "source": [
        "Time series can be **multivariate**, which means there can be many values (= data points) describing one time point. In this task, you will use one **multivariate** dataset (**Eplipsy**) and try to run one deep learning model and one sktime model to see if those models work well on multivariate time series.\n",
        "\n",
        "- Use Eplipsy dataset in the UCR/UEA repository.\n",
        "- Run two classifiers of your choice in sktime, such as TapNet, Rocket, or MiniRocket, together with **the best tensorflow deep learning model from the previous tasks** on Eplipsy. You need to adjust the deep learning model's input layer to handle this multivariate dataset.\n",
        "- Use sktime's `load_UCR_UEA_dataset` function to perform. You should use each dataset's original train/test splits.\n",
        "- For the deep learning model, you should transform it using TensorFlow data API (`tf.data`) to manage your dataset and use `shuffle`, `batch`, and `prefetch` functions. This means that you need to create the validation set first. If you use Torch, explain how you implement the equivalent operations.\n",
        "- For training, you need to run at least 10 epochs for your deep learning model. For TapNet, you can keep the default parameter options.\n",
        "- Report the test scores of three models on the predefined test set.\n",
        "- **Do the same task on one more chosen multivariate time series dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVXhy9vEuDbK"
      },
      "outputs": [],
      "source": [
        "from sktime.datasets import load_UCR_UEA_dataset\n",
        "X_train_epilepsy, y_train_epilepsy = load_UCR_UEA_dataset(name=\"Epilepsy\", split=\"train\", return_X_y=True, return_type=\"numpy3D\")\n",
        "X_test_epilepsy, y_test_epilepsy = load_UCR_UEA_dataset(name=\"Epilepsy\", split=\"test\", return_X_y=True, return_type=\"numpy3D\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqvQ4G6VmFsT"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(X, y, batch_size=32, shuffle_buffer_size=100):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    if shuffle_buffer_size > 0:\n",
        "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycI2w3jShQqw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, Conv1D, GRU, Flatten, Input\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "\n",
        "if not isinstance(y_train_epilepsy, np.ndarray):\n",
        "    y_train_epilepsy = np.array(y_train_epilepsy)\n",
        "if not isinstance(y_test_epilepsy, np.ndarray):\n",
        "    y_test_epilepsy = np.array(y_test_epilepsy)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "y_train_encoded = label_encoder.fit_transform(y_train_epilepsy)\n",
        "y_test_encoded = label_encoder.transform(y_test_epilepsy)\n",
        "\n",
        "y_train_epilepsy = y_train_encoded.astype('int64')\n",
        "y_test_epilepsy = y_test_encoded.astype('int64')\n",
        "\n",
        "def prepare_dataset(X, y, batch_size=32, shuffle_buffer_size=100):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    if shuffle_buffer_size > 0:\n",
        "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1SqY7d4fgKY"
      },
      "outputs": [],
      "source": [
        "def create_fcn_model_multivariate(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Input(shape=input_shape),\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TMSzMosfwlv"
      },
      "outputs": [],
      "source": [
        "train_dataset = prepare_dataset(X_train_epilepsy,y_train_epilepsy)\n",
        "test_dataset = prepare_dataset(X_test_epilepsy, y_test_epilepsy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozGdvcPUgVw6"
      },
      "outputs": [],
      "source": [
        "input_shape = X_train_epilepsy.shape[1:]\n",
        "num_classes = len(np.unique(y_train_epilepsy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Yd8s7VHgc4T",
        "outputId": "4da1a802-e9f8-4565-d7a3-6678bd19e854"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "5/5 [==============================] - 1s 63ms/step - loss: 1.2994 - accuracy: 0.4307 - val_loss: 1.0049 - val_accuracy: 0.5652\n",
            "Epoch 2/10\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.6932 - accuracy: 0.7664 - val_loss: 0.7348 - val_accuracy: 0.7029\n",
            "Epoch 3/10\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.3504 - accuracy: 0.8832 - val_loss: 0.6405 - val_accuracy: 0.7464\n",
            "Epoch 4/10\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.1716 - accuracy: 0.9708 - val_loss: 0.9505 - val_accuracy: 0.6739\n",
            "Epoch 5/10\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0692 - accuracy: 0.9781 - val_loss: 1.0066 - val_accuracy: 0.6739\n",
            "Epoch 6/10\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 0.7376 - val_accuracy: 0.7319\n",
            "Epoch 7/10\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 1.2482 - val_accuracy: 0.6812\n",
            "Epoch 8/10\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.9714 - val_accuracy: 0.6812\n",
            "Epoch 9/10\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.2292 - val_accuracy: 0.6812\n",
            "Epoch 10/10\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.9297 - val_accuracy: 0.7464\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79bab0a8f220>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = create_fcn_model_multivariate(input_shape, num_classes)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_dataset, epochs=10, validation_data=test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTfvHZ0DDtTU",
        "outputId": "3328d5dc-394a-4857-b81a-f5d8a374ea08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 0s 9ms/step - loss: 0.9297 - accuracy: 0.7464\n",
            "Test accuracy: 0.7464\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ytaCT0b0XT7",
        "outputId": "cee78ebc-939b-480c-d3d5-d145bc3fa026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting keras-self-attention\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-self-attention) (1.25.2)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18894 sha256=902e1a2fda477e2853c1b31c27ab932967e6cc25e53f76946229754681c32a27\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/f7/24/607b483144fb9c47b4ba2c5fba6b68e54aeee2d5bf6c05302e\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.51.0\n"
          ]
        }
      ],
      "source": [
        "pip install keras-self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nW_WAUeDgWCa",
        "outputId": "8a6dc809-73b5-4309-9a15-b90beb786493"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 3s 279ms/step\n"
          ]
        }
      ],
      "source": [
        "from sktime.classification.deep_learning import TapNetClassifier\n",
        "\n",
        "tapnet = TapNetClassifier(n_epochs=10)\n",
        "tapnet.fit(X_train_epilepsy, y_train_epilepsy)\n",
        "tapnet_score = tapnet.score(X_test_epilepsy, y_test_epilepsy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uq2qHwJO05vg",
        "outputId": "d41cc8a5-5a10-4c4d-a173-54b460619878"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of TapNet classifier: 0.92\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy of TapNet classifier: {tapnet_score:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtLyMrgnEDBG"
      },
      "outputs": [],
      "source": [
        "from sktime.classification.kernel_based import RocketClassifier\n",
        "rocket = RocketClassifier(n_jobs = -1)\n",
        "rocket.fit(X_train_epilepsy, y_train_epilepsy)\n",
        "rocket_score =rocket.score(X_test_epilepsy, y_test_epilepsy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-0aWGtYEwjc",
        "outputId": "d8246a85-131b-4132-c481-759e5f2849ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Rocket classifier: 0.99\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy of Rocket classifier: {rocket_score:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piIp4cqdFZIK"
      },
      "outputs": [],
      "source": [
        "#Cricket dataset\n",
        "from sktime.datasets import load_UCR_UEA_dataset\n",
        "X_train_cr, y_train_cr = load_UCR_UEA_dataset(name=\"Cricket\", split=\"train\", return_X_y=True, return_type=\"numpy3D\")\n",
        "X_test_cr, y_test_cr = load_UCR_UEA_dataset(name=\"Cricket\", split=\"test\", return_X_y=True, return_type=\"numpy3D\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sktime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJgxBQD39dP8",
        "outputId": "8efa949c-ffcb-4f46-8f19-48951d98375f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sktime\n",
            "  Downloading sktime-0.28.0-py3-none-any.whl (21.9 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m21.9/21.9 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.27,>=1.21 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sktime) (24.0)\n",
            "Requirement already satisfied: pandas<2.3.0,>=1.1 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.5.3)\n",
            "Collecting scikit-base<0.8.0 (from sktime)\n",
            "  Downloading scikit_base-0.7.5-py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m128.8/128.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<1.5.0,>=0.24 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.2.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.11.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2023.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.5.0,>=0.24->sktime) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.5.0,>=0.24->sktime) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<2.3.0,>=1.1->sktime) (1.16.0)\n",
            "Installing collected packages: scikit-base, sktime\n",
            "Successfully installed scikit-base-0.7.5 sktime-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_G0Slx_tjZu"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, Conv1D, GRU, Flatten, Input\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "\n",
        "if not isinstance(y_train_cr, np.ndarray):\n",
        "    y_train_cr = np.array(y_train_cr)\n",
        "if not isinstance(y_test_cr, np.ndarray):\n",
        "    y_test_cr = np.array(y_test_cr)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "y_train_encoded = label_encoder.fit_transform(y_train_cr)\n",
        "y_test_encoded = label_encoder.transform(y_test_cr)\n",
        "\n",
        "y_train_cr = y_train_encoded.astype('int64')\n",
        "y_test_cr = y_test_encoded.astype('int64')\n",
        "\n",
        "def prepare_dataset(X, y, batch_size=32, shuffle_buffer_size=100):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    if shuffle_buffer_size > 0:\n",
        "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juzpbTX-uuAR"
      },
      "outputs": [],
      "source": [
        "mean = X_train_cr.mean()\n",
        "std = X_train_cr.std()\n",
        "\n",
        "X_train_normalized = (X_train_cr - mean) / std\n",
        "X_test_normalized = (X_test_cr - mean) / std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0zdRM6fvEJu",
        "outputId": "f63a0974-94c5-477e-f129-aceeadef1ad6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(108, 6, 1197)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#instances,dimension,timestep\n",
        "X_train_cr.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwKsJcFpu1WW"
      },
      "outputs": [],
      "source": [
        "X_train_cr= X_train_cr.reshape((-1, 1197, 6))\n",
        "X_test_cr = X_test_cr.reshape((-1, 1197, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kU3G3uP6t7db"
      },
      "outputs": [],
      "source": [
        "train_dataset = prepare_dataset(X_train_cr,y_train_cr)\n",
        "test_dataset = prepare_dataset(X_test_cr, y_test_cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rYLcn6AtLVs"
      },
      "outputs": [],
      "source": [
        "input_shape = X_train_cr.shape[1:]\n",
        "num_classes = len(np.unique(y_train_cr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3TqAy_Xyrw-",
        "outputId": "ffc8116b-dc9b-43b5-a664-957b206347fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 2s 123ms/step - loss: 2.2322 - accuracy: 0.1944 - val_loss: 1.3184 - val_accuracy: 0.5556\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 66ms/step - loss: 0.7910 - accuracy: 0.7963 - val_loss: 0.8146 - val_accuracy: 0.7500\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 55ms/step - loss: 0.1837 - accuracy: 0.9722 - val_loss: 0.6676 - val_accuracy: 0.8333\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0507 - accuracy: 0.9907 - val_loss: 0.5704 - val_accuracy: 0.8472\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.0189 - accuracy: 0.9907 - val_loss: 0.7626 - val_accuracy: 0.9028\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 53ms/step - loss: 0.0265 - accuracy: 0.9907 - val_loss: 0.6981 - val_accuracy: 0.8750\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 0.1657 - accuracy: 0.9722 - val_loss: 0.9597 - val_accuracy: 0.8611\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 0.3310 - accuracy: 0.9352 - val_loss: 0.9942 - val_accuracy: 0.7917\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 62ms/step - loss: 0.1077 - accuracy: 0.9722 - val_loss: 0.7054 - val_accuracy: 0.8333\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 77ms/step - loss: 0.0447 - accuracy: 0.9815 - val_loss: 0.9472 - val_accuracy: 0.8611\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c4cc7f35c90>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "model = create_fcn_model_multivariate(input_shape, num_classes)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_dataset, epochs=10, validation_data=test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWdDtsRczF--",
        "outputId": "491a1de9-94c1-4860-8483-e99ecdd0ebcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 16ms/step - loss: 0.9472 - accuracy: 0.8611\n",
            "Test accuracy: 0.8611\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypEEAZgPziJr",
        "outputId": "10544aab-3545-4a91-fdcd-9abfe57609cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 2s 16ms/step\n"
          ]
        }
      ],
      "source": [
        "from sktime.classification.deep_learning import TapNetClassifier\n",
        "\n",
        "tapnet = TapNetClassifier(n_epochs=10)\n",
        "tapnet.fit(X_train_cr, y_train_cr)\n",
        "tapnet_score = tapnet.score(X_test_cr, y_test_cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3CrHBI30iXg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c25938-63e7-4bad-e5b7-cddbe38bec9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of TapNet classifier: 0.40\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy of TapNet classifier: {tapnet_score:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgwfG8Wc0VK4"
      },
      "outputs": [],
      "source": [
        "from sktime.classification.kernel_based import RocketClassifier\n",
        "rocket = RocketClassifier(n_jobs = -1)\n",
        "rocket.fit(X_train_cr, y_train_cr)\n",
        "rocket_score =rocket.score(X_test_cr, y_test_cr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy of Rocket classifier: {rocket_score:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "St4ZUwfT_qqe",
        "outputId": "23093535-434a-4d4f-9628-141747f6c34b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Rocket classifier: 0.38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb9DAP4dGw15"
      },
      "source": [
        "### Put everything together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkaB60luImsS"
      },
      "source": [
        "You have tested various models on your four chosen datasets. Which model shows the best performance? You need to write a simple report which should be max two pages about your trials.\n",
        " - Task 0: State three univariate time series datasets you chose for the tasks.\n",
        " - Task 1: Report the average test scores of four models on three datasets you chose. It would be four scores in total. Mark the best model in terms of the average test score. Briefly explain the structure you constructed.\n",
        " - Task 2: Report test scores of three models on three datasets you chose. It would be nine scores in total. Report the rank of average accuracy scores of three models. Briefly explain two deep learning model structures you constructed to confirm that you correctly developed the models in the paper.\n",
        " - Task 3: Briefly explain which model you chose, how you improved it. Report test scores of the model on three datasets you chose.\n",
        " - Task 4: Explain your choice of dataset and (classifier, transformer) pair and the parameters you tried to optimize. Report the best score and estimator from the randomized search instance and the test score of the best model.  Compare your best score to the score from the same model with the default setting.\n",
        " - Task 5: Report the best model (your two chosen classifiers vs your deep learning model) in terms of the test score. Briefly explain how you handle the multivariate dataset for your two models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JN40RRypdFDC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}